{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Gqkaaqv2bYO"
   },
   "source": [
    "## Creating a 2-D convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pF4RuHke2bYV",
    "outputId": "a1e17ef3-7344-4155-8b51-2a149014c067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "11501568/11490434 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUT76gEu2bYZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_new, X_val, y_train_new, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y11MI7fu2bYa"
   },
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    Simple initialization with Gaussian distribution\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self,  n,c, s):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          Number of nodes in the previous layer\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma *np.random.randn( n,c, s,s)\n",
    "        return W\n",
    "    def B(self, n):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma *np.random.randn(n)\n",
    "        return B\n",
    "    \n",
    "class XavierInitializer:\n",
    "    def __init__(self,sigma):\n",
    "        self.sigma =sigma\n",
    "        \n",
    "    def W(self,  n_nodes1, n_nodes2):\n",
    "      \n",
    "        self.sigma = 1/np.sqrt(n_nodes1)\n",
    "        W = self.sigma *np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self,n_nodes2):\n",
    "        B = self.sigma *np.random.randn(n_nodes2)\n",
    "        return B\n",
    "\n",
    "class HeInitializer:\n",
    "    def __init__(self,sigma):\n",
    "        self.sigma =sigma\n",
    "    def W(self,  n_nodes1, n_nodes2):\n",
    "      \n",
    "        self.sigma = np.sqrt(2/n_nodes1)\n",
    "        W = self.sigma *np.random.randn( n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self,n_nodes2):\n",
    "        B = self.sigma *np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KgNvYqdh2bYc"
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : Learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases for a layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Instance of the layer before update\n",
    "        \"\"\"\n",
    "        layer.W = layer.W - self.lr*layer.dW\n",
    "        layer.B = layer.B - self.lr*layer.dB\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "o1IX3Hw52bYd"
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, A): \n",
    "        Z = np.exp(A) / np.sum(np.exp(A), axis=1).reshape(-1, 1)\n",
    "        return Z\n",
    "        \n",
    "    def backward(self, Z, y):\n",
    "        dA = Z - y\n",
    "\n",
    "        return dA\n",
    "    def loss(self,Z,y):\n",
    "        L = - np.sum(y * np.log(Z)) / len(y)\n",
    "        return L\n",
    "    \n",
    "class ReLU:\n",
    "    def forward(self, A): \n",
    "        self.A = A\n",
    "        A[A <= 0] = 0\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dA = dZ*np.array(self.A > 0, np.int)\n",
    "        return dA\n",
    "    \n",
    "class Sigmoid:\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = 1 / (1 + np.exp(-self.A))\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * ((1 / (1 + np.exp(-self.A))) - (1 / (1 + np.exp(-self.A)))**2)\n",
    "        return dA\n",
    "    \n",
    "class Tanh:\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = np.tanh(self.A)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * (1 - np.tanh(self.A)**2)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T7YLIpq42bYf"
   },
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      Number of nodes in the previous layer\n",
    "    n_nodes2 : int\n",
    "      Number of nodes in the later layer\n",
    "    initializer: instance of initialization method\n",
    "    optimizer: instance of optimization method\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # Initialize\n",
    "        # Initialize self.W and self.B using the initializer method\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.W = initializer.W(n_nodes1,n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.Hw = 0\n",
    "        self.Hb = 0\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
    "            output\n",
    "        \"\"\"        \n",
    "#         print(X.shape)\n",
    "        A =  X @self.W + self.B\n",
    "        self.Z = X\n",
    "        return A\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient flowing from behind\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
    "            Gradient to flow forward\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA,axis = 0)\n",
    "        self.dW = self.Z.T@dA\n",
    "        self.dZ = dA @ self.W.T\n",
    "        \n",
    "        # update\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EGd_r2ZA2bZE"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Conv2d:\n",
    " \n",
    "    def __init__(self, initializer, optimizer, filter_num, C, filter_size, stride=1, pad=0):\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.initializer = initializer\n",
    "        self.W = self.initializer.W(n=filter_num, c=C, s=filter_size)\n",
    "        self.B = self.initializer.B(filter_num)\n",
    "        \n",
    "        \n",
    "        self.X = None \n",
    "        self.col = None \n",
    "        self.col_W = None \n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    " \n",
    "        FN, C, FH, FW = self.W.shape \n",
    "        N, C, H, W = X.shape \n",
    "        \n",
    "        \n",
    "        out_h, out_w = self._out_shape(H, FH, W, FW)\n",
    "        \n",
    "        \n",
    "        self.col = self._im2col(X, FH, FW, self.stride, self.pad)\n",
    "        \n",
    "        \n",
    "        self.col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        \n",
    "#         A = np.dot(self.col, self.col_W) + self.B\n",
    "        A = self.col@ self.col_W + self.B\n",
    "        A = A.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) \n",
    "        \n",
    "        \n",
    "        self.X = X\n",
    "        \n",
    "        return A\n",
    "\n",
    "\n",
    "    def backward(self, dA):\n",
    "\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        \n",
    "        \n",
    "        dA = dA.transpose(0,2,3,1).reshape(-1, FN) \n",
    "        \n",
    "        \n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dA)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "        dcol = np.dot(dA, self.col_W.T)\n",
    "        \n",
    "        \n",
    "        dX = self._col2im(dcol, self.X.shape, FH, FW, self.stride, self.pad)\n",
    "        \n",
    "        \n",
    "        self = self.optimizer.update(self)        \n",
    "\n",
    "        return dX\n",
    "    \n",
    "    \n",
    "    def _im2col(self, X, FH, FW, stride=1, pad=0):\n",
    "\n",
    "        N, C, H, W = X.shape\n",
    "        \n",
    "        \n",
    "        out_h, out_w = self._out_shape(H, FH, W, FW)\n",
    "\n",
    "        \n",
    "        img = np.pad(X, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "        \n",
    "        \n",
    "        col = np.zeros((N, C, FH, FW, out_h, out_w))\n",
    "        for y in range(FH):\n",
    "            y_max = y + stride * out_h\n",
    "            for x in range(FW):\n",
    "                x_max = x + stride * out_w\n",
    "                col[:, :, y, x, :, :] = img[:, :, y: y_max: stride, x: x_max: stride]\n",
    "        \n",
    "        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n",
    "\n",
    "        return col\n",
    "    \n",
    "    \n",
    "    def _col2im(self, dcol, X_shape, FH, FW, stride=1, pad=0):\n",
    " \n",
    "        N, C, H, W = X_shape\n",
    "\n",
    "       \n",
    "        out_h, out_w = self._out_shape(H, FH, W, FW)\n",
    "        \n",
    "        \n",
    "        dcol = dcol.reshape(N, out_h, out_w, C, FH, FW).transpose(0, 3, 4, 5, 1, 2)\n",
    "        \n",
    "       \n",
    "        img = np.zeros((N, C, H + 2 * pad + stride - 1, W + 2 * pad + stride - 1))\n",
    "        for y in range(FH):\n",
    "            y_max = y + stride * out_h\n",
    "            for x in range(FW):\n",
    "                x_max = x + stride * out_w\n",
    "                img[:, :, y: y_max: stride, x: x_max: stride] += dcol[:, :, y, x, :, :]\n",
    "\n",
    "        return img[:, :, pad: H + pad, pad: W + pad]\n",
    "    \n",
    "    \n",
    "    def _out_shape(self, H, FH, W, FW):\n",
    "\n",
    "        out_h = 1 + int((H + 2 * self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2 * self.pad - FW) / self.stride)\n",
    "        \n",
    "        return out_h, out_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s31msapF2bZG"
   },
   "source": [
    "## Experiments with 2D convolutional layers on small arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DxM6hcz2bZH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# CNN2 のフォワードを流す時の入力データ\n",
    "# (1,1,4,4)\n",
    "x = np.array([[[[ 1,  2,  3,  4],\n",
    "                [ 5,  6,  7,  8],\n",
    "                [ 9, 10, 11, 12],\n",
    "                [13, 14, 15, 16]]]])\n",
    "\n",
    "# (2,3,3)\n",
    "w = np.array([[[ 0.,  0.,  0.],\n",
    "               [ 0.,  1.,  0.],\n",
    "               [ 0., -1.,  0.]],\n",
    "\n",
    "              [[ 0.,  0.,  0.],\n",
    "               [ 0., -1.,  1.],\n",
    "               [ 0.,  0.,  0.]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8cfresx2bZI"
   },
   "outputs": [],
   "source": [
    "w = np.expand_dims(w,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxDyZI0D2bZJ"
   },
   "outputs": [],
   "source": [
    "test = Conv2d(filter_num=2, filter_size=3,C=1, stride=1, pad=0, initializer=SimpleInitializer(0.2),optimizer=SGD(0.1))\n",
    "# testing = test.forward(mini_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChutXG8g2bZL"
   },
   "outputs": [],
   "source": [
    "test.W = w\n",
    "test.B = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rlks4hr2bZM"
   },
   "outputs": [],
   "source": [
    "result = test.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ottRx0gi2bZM",
    "outputId": "dc5d42e5-e382-4644-d132-47d01571b573"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-4., -4.],\n",
       "         [-4., -4.]],\n",
       "\n",
       "        [[ 1.,  1.],\n",
       "         [ 1.,  1.]]]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3fKYyV-2bZQ"
   },
   "outputs": [],
   "source": [
    "delta = np.array([[[ -4,  -4],\n",
    "                   [ 10,  11]],\n",
    "\n",
    "                  [[  1,  -7],\n",
    "                   [  1, -11]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VlDGlcrr2bZS"
   },
   "outputs": [],
   "source": [
    "delta = np.expand_dims(delta,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPzW1a8y2bZS",
    "outputId": "99f9be86-9d65-4239-a40f-8fb4aa2fb446"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,  -5.,  11.],\n",
       "         [  0.,  12.,  -5., -11.],\n",
       "         [  0.,  -1.,  -1.,   0.]]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.backward(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss5D-U-e2bZT"
   },
   "source": [
    "## Creation of maximum pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nu7Sq2pl2bZU"
   },
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "\n",
    "    def __init__(self, pool_h=3, pool_w=3, stride=1, pad=0):\n",
    "       \n",
    "        self.pool_h = pool_h \n",
    "        self.pool_w = pool_w \n",
    "        self.stride = stride \n",
    "        self.pad = pad \n",
    "        \n",
    "       \n",
    "        self.X = None \n",
    "        self.arg_max = None \n",
    "\n",
    "    def forward(self, X):\n",
    " \n",
    "        N, C, H, W = X.shape\n",
    "        \n",
    "        \n",
    "        out_h, out_w = self._out_shape(H, self.pool_h, W, self.pool_w)\n",
    "        \n",
    "        \n",
    "        col = self._im2col(X, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "        \n",
    "        \n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "#         print(col)\n",
    "#         print(np.max(col, axis=1))\n",
    "#         print(np.argmax(col, axis=1).size)\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2) #(N, out_h, out_w, C)→(N, C, out_h, out_w)\n",
    "\n",
    "        \n",
    "        self.X = X\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dX = self._col2im(dcol, self.X.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    def _im2col(self, input_data, FH, FW, stride=1, pad=0):\n",
    "\n",
    "        N, C, H, W = input_data.shape\n",
    "        \n",
    "       \n",
    "        out_h, out_w = self._out_shape(H, FH, W, FW)\n",
    "\n",
    "        \n",
    "        img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "        \n",
    "       \n",
    "        col = np.zeros((N, C, FH, FW, out_h, out_w))\n",
    "        for y in range(FH):\n",
    "            y_max = y + stride * out_h\n",
    "            for x in range(FW):\n",
    "                x_max = x + stride * out_w\n",
    "                col[:, :, y, x, :, :] = img[:, :, y: y_max: stride, x: x_max: stride]\n",
    "\n",
    "        \n",
    "        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1) \n",
    "\n",
    "        return col\n",
    "    \n",
    "    \n",
    "    def _col2im(self, col, input_shape, FH, FW, stride=1, pad=0):\n",
    "\n",
    "        N, C, H, W = input_shape\n",
    "\n",
    "        \n",
    "        out_h, out_w = self._out_shape(H, FH, W, FW)\n",
    "        \n",
    "        \n",
    "        col = col.reshape(N, out_h, out_w, C, FH, FW).transpose(0, 3, 4, 5, 1, 2) \n",
    "\n",
    "       \n",
    "        img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "        for y in range(FH):\n",
    "            y_max = y + stride*out_h\n",
    "            for x in range(FW):\n",
    "                x_max = x + stride*out_w\n",
    "                img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "        return img[:, :, pad:H + pad, pad:W + pad]\n",
    "    \n",
    "    \n",
    "    def _out_shape(self, H, FH, W, FW):\n",
    "\n",
    "        out_h = 1 + int((H - FH) / self.stride)\n",
    "        out_w = 1 + int((W- FW) / self.stride)\n",
    "        \n",
    "        return out_h, out_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2fS0yGL2bZX"
   },
   "source": [
    "## Creating average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6OEvHUt32bZX"
   },
   "outputs": [],
   "source": [
    "class AveragePool2D:\n",
    "\n",
    "    def __init__(self, pool_h=3, pool_w=3, stride=1, pad=0):\n",
    "       \n",
    "        self.pool_h = pool_h \n",
    "        self.pool_w = pool_w \n",
    "        self.stride = stride \n",
    "        self.pad = pad \n",
    "        \n",
    "       \n",
    "        self.X = None \n",
    "#         self.arg_max = None \n",
    "\n",
    "    def forward(self, X):\n",
    " \n",
    "        N, C, H, W = X.shape\n",
    "        \n",
    "        \n",
    "        out_h, out_w = self._out_shape(H, self.pool_h, W, self.pool_w)\n",
    "        \n",
    "        \n",
    "        col = self._im2col(X, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
    "        \n",
    "        \n",
    "\n",
    "        out = np.mean(col, axis=1)\n",
    "        self.length = col.shape[0]\n",
    "\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2) #(N, out_h, out_w, C)→(N, C, out_h, out_w)\n",
    "\n",
    "        \n",
    "        self.X = X\n",
    "        # self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        davg = np.zeros((dout.size, pool_size))\n",
    "        flatten = dout.flatten()\n",
    "        for i in range(self.length):\n",
    "            davg[i,:] = flatten[i]/len(davg[i,:])\n",
    "\n",
    "        davg = davg.reshape(dout.shape + (pool_size,)) \n",
    "        dcol = davg.reshape(davg.shape[0] * davg.shape[1] * davg.shape[2], -1)\n",
    "        dX = self._col2im(dcol, self.X.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    def _im2col(self, input_data, FH, FW, stride=1, pad=0):\n",
    "\n",
    "        N, C, H, W = input_data.shape\n",
    "        \n",
    "       \n",
    "        out_h, out_w = self._out_shape(H, FH, W, FW)\n",
    "\n",
    "        \n",
    "        img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "        \n",
    "       \n",
    "        col = np.zeros((N, C, FH, FW, out_h, out_w))\n",
    "        for y in range(FH):\n",
    "            y_max = y + stride * out_h\n",
    "            for x in range(FW):\n",
    "                x_max = x + stride * out_w\n",
    "                col[:, :, y, x, :, :] = img[:, :, y: y_max: stride, x: x_max: stride]\n",
    "\n",
    "        \n",
    "        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1) \n",
    "\n",
    "        return col\n",
    "    \n",
    "    \n",
    "    def _col2im(self, col, input_shape, FH, FW, stride=1, pad=0):\n",
    "\n",
    "        N, C, H, W = input_shape\n",
    "\n",
    "        \n",
    "        out_h, out_w = self._out_shape(H, FH, W, FW)\n",
    "        \n",
    "        \n",
    "        col = col.reshape(N, out_h, out_w, C, FH, FW).transpose(0, 3, 4, 5, 1, 2) \n",
    "\n",
    "       \n",
    "        img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "        for y in range(FH):\n",
    "            y_max = y + stride*out_h\n",
    "            for x in range(FW):\n",
    "                x_max = x + stride*out_w\n",
    "                img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "        return img[:, :, pad:H + pad, pad:W + pad]\n",
    "    \n",
    "    \n",
    "    def _out_shape(self, H, FH, W, FW):\n",
    "\n",
    "        out_h = 1 + int((H - FH) / self.stride)\n",
    "        out_w = 1 + int((W- FW) / self.stride)\n",
    "        \n",
    "        return out_h, out_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m97DUQFA2bZZ"
   },
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YLkIp4fq2bZa"
   },
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.X_shape = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        X_1d = X.reshape(X.shape[0], -1)\n",
    "        \n",
    "      \n",
    "        self.X_shape = X.shape\n",
    "        \n",
    "        return X_1d\n",
    "    \n",
    "\n",
    "    def backward(self, X):\n",
    "        \n",
    "        X = X.reshape(self.X_shape)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4il93W8K2bZa"
   },
   "source": [
    "## Learning and estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZeTWysTD2bZb"
   },
   "outputs": [],
   "source": [
    "class ScratchConv2Classifier():\n",
    "    \"\"\"\n",
    "    Simple three-layer neural network classifier\n",
    "    Parameters\n",
    "    ----------\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self,filter_num, C, filter_size,n_output,stride=1, pad=0,batch_size = 20 ,epochs=10,sigma=0.02,optimizer=SGD,activation_function = \"sigmoid\",lr = 0.01,bias = False,verbose = True):\n",
    "        self.verbose = verbose\n",
    "        self.filter_size= filter_size\n",
    "        self.filter_num = filter_num\n",
    "        self.C = C \n",
    "        self.batch_size = batch_size\n",
    "        self.n_output = n_output \n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.check_bias = bias\n",
    "        self.activation_function = activation_function\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optimizer(self.lr)\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        self.initializer = self.get_initializer()\n",
    "\n",
    "        self.conv= Conv2d(filter_num =self.filter_num , filter_size = self.filter_size,C =X.shape[1], stride= self.stride, pad =self.pad, initializer=SimpleInitializer(sigma=0.02),optimizer=self.optimizer)\n",
    "        self.pool = MaxPool2D(pool_h= 2,pool_w =2)\n",
    "        self.flatten = Flatten()\n",
    "        out_h, out_w = self._out_shape(X.shape[-2], self.filter_size, X.shape[-1], self.filter_size,self.pad,self.stride)\n",
    "        out_h, out_w = self._out_shape(out_h, 2, out_w, 2, self.pool.pad, self.pool.stride)\n",
    "        nodes = self.filter_num * out_h * out_w \n",
    "        self.FC = FC(nodes,self.n_output,self.initializer,self.optimizer)\n",
    "        \n",
    "#         self.conv1d = Conv1d(kernel_size=7, initializer=SimpleInitializer(self.sigma), optimizer=self.optimizer, input_channels=1, output_channels=1, padding=3, stride=2)\n",
    "#         nodes_1 = output_size_calculation(X.shape[-1], self.conv1d.padding, self.conv1d.kernel_size, self.conv1d.stride)\n",
    "#         self.FC2 = FC(nodes_1,self.n_nodes2,self.initializer,self.optimizer)\n",
    "#         self.FC3 = FC(self.n_nodes2, self.n_output,self.initializer,self.optimizer)\n",
    "        \n",
    "        self.activation1 = self.get_activation()\n",
    "#         self.activation2 = self.get_activation()\n",
    "        self.activation2 = Softmax()\n",
    "            \n",
    "        get_mini_batch = GetMiniBatch(X,y,self.batch_size)\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "        for epoch in range(self.epochs):  \n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "\n",
    "                self.forward(mini_X_train)\n",
    "#                \n",
    "                self.backward(mini_X_train,mini_y_train)\n",
    "#             break\n",
    "            self.forward(X)\n",
    "            self.loss_train.append(self.activation3.loss(self.result,y))\n",
    "            if X_val is not None:\n",
    "                self.forward(X_val)\n",
    "                self.loss_val.append(self.activation3.loss(self.result,y_val))\n",
    "            \n",
    "\n",
    "    \n",
    "        \n",
    "        \"\"\"\n",
    "        Learn a neural network classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of training data\n",
    "        y : The following form of ndarray, shape (n_samples,)\n",
    "            Correct answer value of training data\n",
    "        X_val : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of verification data\n",
    "        y_val : The following form of ndarray, shape (n_samples,)\n",
    "            Correct value of verification data\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.verbose:\n",
    "             #verbose is set to True, the learning process etc. is output.\n",
    "            print(self.loss_train)\n",
    "    def get_initializer(self):\n",
    "        if self.activation_function == \"sigmoid\" or self.activation_function == \"tanh\":\n",
    "            return XavierInitializer(self.sigma)\n",
    "        elif self.activation_function == \"relu\":\n",
    "            return HeInitializer(self.sigma)\n",
    "    \n",
    "    def get_activation(self):\n",
    "        if self.activation_function == \"sigmoid\" :\n",
    "            return Sigmoid()\n",
    "        elif self.activation_function == \"tanh\":\n",
    "            return Tanh()\n",
    "        elif self.activation_function == \"relu\":\n",
    "            return ReLU()\n",
    "\n",
    "    def _out_shape(self, H, FH, W, FW,pad,stride):\n",
    "\n",
    "        out_h = 1 + int((H + 2 * pad - FH) / stride)\n",
    "        out_w = 1 + int((W + 2 * pad - FW) / stride)\n",
    "        \n",
    "        return out_h, out_w\n",
    "    \n",
    "        \n",
    "       \n",
    "    def forward(self,X):\n",
    "        A1 = self.conv.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.pool.forward(Z1)\n",
    "        Z2 = self.flatten.forward(A2)\n",
    "\n",
    "        A3 = self.FC.forward(Z2)\n",
    "\n",
    "        Z3 = self.activation2.forward(A3)\n",
    "        self.result = Z3\n",
    "\n",
    "        \n",
    "\n",
    "    def backward(self,X,y):\n",
    "#         print(self.result.shape)\n",
    "#         print(y.shape)\n",
    "        dA3 = self.activation2.backward(self.result,y)\n",
    "        dZ2 = self.FC.backward(dA3)\n",
    "        dA2 = self.flatten.backward(dZ2)\n",
    "        dZ1 = self.pool.backward(dA2)\n",
    "        dA1 = self.activation1.backward(dZ1)\n",
    "        dZ0 = self.conv.backward(dA1)\n",
    "       \n",
    "        \n",
    "\n",
    "    def derivative(self,A,d):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            dA = d*(1 / (1 + np.exp(-A)))*(1-(1 / (1 + np.exp(-A))))\n",
    "            return dA\n",
    "        elif self.activation_function == \" tanh\":\n",
    "            dA = d*(1- np.tanh(A)**2)\n",
    "            return dA\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Estimate using a neural network classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            sample\n",
    "        Returns\n",
    "        -------\n",
    "            The following form of ndarray, shape (n_samples, 1)\n",
    "            Estimated result\n",
    "        \"\"\"\n",
    "        self.forward(X)\n",
    "        return np.argmax(self.result,axis = 1)\n",
    "        \n",
    "      \n",
    "    \n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "Iterator to get a mini-batch\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "      Training data\n",
    "    y : The following form of ndarray, shape (n_samples, 1)\n",
    "      Correct answer value\n",
    "    batch_size : int\n",
    "      Batch size\n",
    "    seed : int\n",
    "      NumPy random number seed\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1gPhxxq2bZf",
    "outputId": "3053bfb9-c934-4622-b4e4-862df020d24a"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "(X_train2, y_train2), (X_test2, y_test2) = mnist.load_data()\n",
    "\n",
    "X_train2 = X_train2.astype(np.float)\n",
    "X_test2 = X_test2.astype(np.float)\n",
    "X_train2 /= 255 \n",
    "X_test2 /= 255\n",
    "\n",
    "X_train2 = X_train2[:, np.newaxis, :, :] \n",
    "X_test2 = X_test2[:, np.newaxis, :, :]\n",
    "\n",
    "X_train2, X_val2, y_train2, y_val2 = train_test_split(X_train2, y_train2, test_size=0.2)\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot2 = enc.fit_transform(y_train2[:, np.newaxis])\n",
    "y_test_one_hot2 = enc.transform(y_val2[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bs-tMn_o2bZk",
    "outputId": "d0054583-dffa-46e6-cb3d-81b53f41a755"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 13.7 GiB for an array with shape (48000, 1, 7, 7, 28, 28) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-41d09731f5a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScratchConv2Classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_one_hot2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_one_hot2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-e670ea0bd222>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_X_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmini_y_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;31m#             break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mX_val\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-e670ea0bd222>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mA1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mZ1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mA2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-416a31aabe4c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_im2col\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-416a31aabe4c>\u001b[0m in \u001b[0;36m_im2col\u001b[1;34m(self, X, FH, FW, stride, pad)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0my_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstride\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mout_h\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 13.7 GiB for an array with shape (48000, 1, 7, 7, 28, 28) and data type float64"
     ]
    }
   ],
   "source": [
    "model = ScratchConv2Classifier(5,3, 7,10, stride=1, pad=3, epochs =5, verbose=False)\n",
    "model.fit(X_train2, y_train_one_hot2, X_val2, y_test_one_hot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Wxcu2q42bZm"
   },
   "outputs": [],
   "source": [
    "result = model.predict(X_test2[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jazBxQCp2bZn"
   },
   "outputs": [],
   "source": [
    "acc = (result == y_test[0:100]).sum()*100/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yhyK7GD2bZn"
   },
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v-8fPyx2bZq"
   },
   "source": [
    "## LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "km7FXqaL2bZr"
   },
   "outputs": [],
   "source": [
    "class LeNet():\n",
    "    \"\"\"\n",
    "    Simple three-layer neural network classifier\n",
    "    Parameters\n",
    "    ----------\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self,filter_num=6, filter_size =5,n_output=10,stride=1, pad=2,batch_size = 20 ,epochs=10,sigma=0.02,optimizer=SGD,activation_function = \"sigmoid\",lr = 0.01,bias = False,verbose = False):\n",
    "        self.verbose = verbose\n",
    "        self.filter_size= filter_size\n",
    "        self.filter_num = filter_num\n",
    "        # self.C = C \n",
    "        self.batch_size = batch_size\n",
    "        self.n_output = n_output \n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.check_bias = bias\n",
    "        self.activation_function = activation_function\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optimizer(self.lr)\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        self.initializer = self.get_initializer()\n",
    "\n",
    "        self.conv1= Conv2d(filter_num =self.filter_num , filter_size = self.filter_size,C =X.shape[1], stride= self.stride, pad =self.pad, initializer=SimpleInitializer(sigma=0.02),optimizer=self.optimizer)\n",
    "        self.pool1 = AveragePool2D(pool_h= 2,pool_w =2,stride =2)\n",
    "        self.conv2 = Conv2d(filter_num =16 , filter_size = self.filter_size,C =6, stride= 1, pad =0, initializer=SimpleInitializer(sigma=0.02),optimizer=self.optimizer)\n",
    "        self.pool2 = AveragePool2D(pool_h= 2,pool_w =2,stride =2)\n",
    "        self.flatten = Flatten()\n",
    "        # out_h, out_w = self._out_shape(X.shape[-2], self.filter_size, X.shape[-1], self.filter_size,self.pad,self.stride)\n",
    "        # out_h, out_w = self._out_shape(out_h, 2, out_w, 2, self.pool.pad, self.pool.stride)\n",
    "        self.FC1 = FC(400,120,self.initializer,self.optimizer)\n",
    "        self.FC2 = FC(120,84,self.initializer,self.optimizer)\n",
    "        self.FC3 = FC(84,self.n_output,self.initializer,self.optimizer)\n",
    "        \n",
    "#         self.conv1d = Conv1d(kernel_size=7, initializer=SimpleInitializer(self.sigma), optimizer=self.optimizer, input_channels=1, output_channels=1, padding=3, stride=2)\n",
    "#         nodes_1 = output_size_calculation(X.shape[-1], self.conv1d.padding, self.conv1d.kernel_size, self.conv1d.stride)\n",
    "#         self.FC2 = FC(nodes_1,self.n_nodes2,self.initializer,self.optimizer)\n",
    "#         self.FC3 = FC(self.n_nodes2, self.n_output,self.initializer,self.optimizer)\n",
    "        \n",
    "        self.activation1 = self.get_activation()\n",
    "        self.activation2 = self.get_activation()\n",
    "        self.activation3 = self.get_activation()\n",
    "        self.activation4 = self.get_activation()\n",
    "        self.activation5 = Softmax()\n",
    "            \n",
    "        get_mini_batch = GetMiniBatch(X,y,self.batch_size)\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "        for epoch in range(self.epochs):  \n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "\n",
    "                self.forward(mini_X_train)\n",
    "#                \n",
    "                self.backward(mini_X_train,mini_y_train)\n",
    "#             break\n",
    "            self.forward(X)\n",
    "            self.loss_train.append(self.activation3.loss(self.result,y))\n",
    "            if X_val is not None:\n",
    "                self.forward(X_val)\n",
    "                self.loss_val.append(self.activation3.loss(self.result,y_val))\n",
    "            \n",
    "\n",
    "    \n",
    "        \n",
    "        \"\"\"\n",
    "        Learn a neural network classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of training data\n",
    "        y : The following form of ndarray, shape (n_samples,)\n",
    "            Correct answer value of training data\n",
    "        X_val : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of verification data\n",
    "        y_val : The following form of ndarray, shape (n_samples,)\n",
    "            Correct value of verification data\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.verbose:\n",
    "             #verbose is set to True, the learning process etc. is output.\n",
    "            print(self.loss_train)\n",
    "    def get_initializer(self):\n",
    "        if self.activation_function == \"sigmoid\" or self.activation_function == \"tanh\":\n",
    "            return XavierInitializer(self.sigma)\n",
    "        elif self.activation_function == \"relu\":\n",
    "            return HeInitializer(self.sigma)\n",
    "    \n",
    "    def get_activation(self):\n",
    "        if self.activation_function == \"sigmoid\" :\n",
    "            return Sigmoid()\n",
    "        elif self.activation_function == \"tanh\":\n",
    "            return Tanh()\n",
    "        elif self.activation_function == \"relu\":\n",
    "            return ReLU()\n",
    "\n",
    "    def _out_shape(self, H, FH, W, FW,pad,stride):\n",
    "\n",
    "        out_h = 1 + int((H + 2 * pad - FH) / stride)\n",
    "        out_w = 1 + int((W + 2 * pad - FW) / stride)\n",
    "        \n",
    "        return out_h, out_w\n",
    "    \n",
    "        \n",
    "       \n",
    "    def forward(self,X):\n",
    "        A1 = self.conv1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.pool1.forward(Z1)\n",
    "        Z2 = self.conv2.forward(A2)\n",
    "        A3 = self.activation2.forward(Z2)\n",
    "        Z3 = self.pool2.forward(A3)\n",
    "        A4 = self.flatten.forward(Z3)\n",
    "        Z4 = self.FC1.forward(A4)\n",
    "        A5 = self.activation3.forward(Z4)\n",
    "        Z5 = self.FC2.forward(A5)\n",
    "        A6 = self.activation4.forward(Z5)\n",
    "        Z6 = self.FC3.forward(A6)\n",
    "        A7 = self.activation5.forward(Z6)\n",
    "\n",
    "        self.result = A7\n",
    "\n",
    "        \n",
    "\n",
    "    def backward(self,X,y):\n",
    "\n",
    "        dZ6 = self.activation5.backward(self.result,y)\n",
    "        dA6 = self.FC3.backward(dZ6)\n",
    "        dZ5 = self.activation4.backward(dA6)\n",
    "        dA5 = self.FC2.backward(dZ5)\n",
    "        dZ4 = self.activation3.backward(dA5)\n",
    "        dA4 = self.FC1.backward(dZ4)\n",
    "        dZ3 = self.flatten.backward(dA4)\n",
    "        dA3 = self.pool2.backward(dZ3)\n",
    "        dZ2 = self.activation2.backward(dA3)\n",
    "        dA2 = self.conv2.backward(dZ2)\n",
    "        dZ1 = self.pool1.backward(dA2)\n",
    "        dA1 = self.activation1.backward(dZ1)\n",
    "        dZ0 = self.conv1.backward(dA1)\n",
    "    \n",
    "       \n",
    "        \n",
    "\n",
    "    def derivative(self,A,d):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            dA = d*(1 / (1 + np.exp(-A)))*(1-(1 / (1 + np.exp(-A))))\n",
    "            return dA\n",
    "        elif self.activation_function == \" tanh\":\n",
    "            dA = d*(1- np.tanh(A)**2)\n",
    "            return dA\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Estimate using a neural network classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            sample\n",
    "        Returns\n",
    "        -------\n",
    "            The following form of ndarray, shape (n_samples, 1)\n",
    "            Estimated result\n",
    "        \"\"\"\n",
    "        self.forward(X)\n",
    "        return np.argmax(self.result,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HvVr0ipKB55U"
   },
   "outputs": [],
   "source": [
    "model = LeNet(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K8sCzrpuCB8m",
    "outputId": "771dda70-4792-4939-9823-f8ef1169f313"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:178: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train2, y_train_one_hot2, X_val2, y_test_one_hot2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFge47TyDn98"
   },
   "source": [
    "## Survey of famous image recognition models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3kmgrVwHJxN"
   },
   "source": [
    "- AlexNet: AlexNet is the name of a convolutional neural network (CNN) architecture, designed by Alex Krizhevsky in collaboration with Ilya Sutskever and Geoffrey Hinton, who was Krizhevsky's Ph.D. advisor.AlexNet competed in the ImageNet Large Scale Visual Recognition Challenge on September 30, 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points lower than that of the runner up. The original paper's primary result was that the depth of the model was essential for its high performance, which was computationally expensive, but made feasible due to the utilization of graphics processing units (GPUs) during training\n",
    "- VGG16: VGG16 is a simple and widely used Convolutional Neural Network (CNN) Architecture used for ImageNet, a large visual database project used in visual object recognition software research. The VGG16 Architecture was developed and introduced by Karen Simonyan and Andrew Zisserman from the University of Oxford, in the year 2014, through their article “Very Deep Convolutional Networks for Large-Scale Image Recognition.” ‘VGG’ is the abbreviation for Visual Geometry Group, which is a group of researchers at the University of Oxford who developed this architecture, and ‘16’ implies that this architecture has 16 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4RI-EEuLqz-"
   },
   "source": [
    "## Calculation of output size and number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nD9VvjdFCeMx"
   },
   "outputs": [],
   "source": [
    "def calc_outshape_parameter(H, FH, W, FW, FC, P, S, IC):\n",
    "    FN = int(FC/IC)\n",
    "    out_h = int((H + 2 * P - FH) / S) + 1\n",
    "    out_w = int((W + 2 * P - FW) / S) + 1\n",
    "    parameter = FH * FW * IC * FN + FN\n",
    "        \n",
    "    return out_h, out_w, parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zTwz3orfL9Jg",
    "outputId": "cdc59a5e-ab3f-40e0-bb80-c31f666a17a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142, 142, 56)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_outshape_parameter(144,3,144,3,6,0,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dz5HeTIzMtzb",
    "outputId": "c0de8b54-45bb-4e56-9496-7866d2b7c3c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 58, 434)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_outshape_parameter(60,3,60,3,48,0,1,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_X5PcwWwNgwM",
    "outputId": "f6a4cf71-85d7-4ed3-8a83-34d33d42f903"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9, 182)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_outshape_parameter(20,3,20,3,20,0,2,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pl5QbJEgNq2r"
   },
   "source": [
    "## Survey on filter size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TD1Di_RPN6Na"
   },
   "source": [
    "### Why 3×3 filters are commonly used instead of larger ones such as 7×7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbomKhFEN-f1"
   },
   "source": [
    "- Less filter less computation, big filter more computation.\n",
    "\n",
    "- It learns large complex features easily, where as large filters learns simple features.\n",
    "\n",
    "- Output Layers will be less when we use 3x3 filters as compared to 5x5 or bigger filters.\n",
    "\n",
    "- Also since there will be more output layers when using 3x3 filters more memory will be required to store them as compared to 5x5 or bigger filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3d9ErGYOE_3"
   },
   "source": [
    "### The effect of a 1 x 1 filter with no height or width direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZWLCQRXOaif"
   },
   "source": [
    "- 1 x 1 filter is used when we want to keep the height and width of input through layers but change the number of channel"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SimpleConv2d.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

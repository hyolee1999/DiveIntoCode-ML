{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCiGGHv5-9sI"
      },
      "source": [
        "##  Looking back on the scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2GjP2Qb-9sb"
      },
      "source": [
        "What you needed to implement deep learning:\n",
        "- Had to initialize the weights\n",
        "- Needed an epoch loop\n",
        "- Implement Convolution layer , Activation layer , Loss function\n",
        "- Choose hyper parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwRp9Zt9-9sd"
      },
      "source": [
        "## Consider the correspondence between scratch and TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5hOTyVK_dSu",
        "outputId": "975d647b-fdfa-4d27-ac84-986cda106906"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apYxjB40-9sf",
        "outputId": "1406c237-b3b1-472f-9f9a-31c86d8ffb7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 0, loss : 5.4193, val_loss : 57.2231, acc : 0.375\n",
            "Epoch 1, loss : 3.5689, val_loss : 34.6412, acc : 0.375\n",
            "Epoch 2, loss : 2.1442, val_loss : 15.9005, acc : 0.188\n",
            "Epoch 3, loss : 1.7802, val_loss : 11.7099, acc : 0.062\n",
            "Epoch 4, loss : 1.4782, val_loss : 11.4397, acc : 0.188\n",
            "Epoch 5, loss : 1.2006, val_loss : 12.0541, acc : 0.250\n",
            "Epoch 6, loss : 1.0606, val_loss : 8.0987, acc : 0.188\n",
            "Epoch 7, loss : 0.9051, val_loss : 5.9181, acc : 0.312\n",
            "Epoch 8, loss : 0.7778, val_loss : 5.5638, acc : 0.312\n",
            "Epoch 9, loss : 0.6314, val_loss : 4.3611, acc : 0.312\n",
            "Epoch 10, loss : 0.4857, val_loss : 2.7496, acc : 0.438\n",
            "Epoch 11, loss : 0.3648, val_loss : 2.1538, acc : 0.438\n",
            "Epoch 12, loss : 0.2902, val_loss : 1.5546, acc : 0.438\n",
            "Epoch 13, loss : 0.2336, val_loss : 1.0378, acc : 0.562\n",
            "Epoch 14, loss : 0.1847, val_loss : 0.9605, acc : 0.688\n",
            "Epoch 15, loss : 0.1568, val_loss : 0.7830, acc : 0.688\n",
            "Epoch 16, loss : 0.1283, val_loss : 0.5272, acc : 0.812\n",
            "Epoch 17, loss : 0.1076, val_loss : 0.3977, acc : 0.875\n",
            "Epoch 18, loss : 0.0932, val_loss : 0.2476, acc : 0.875\n",
            "Epoch 19, loss : 0.0830, val_loss : 0.1727, acc : 0.938\n",
            "Epoch 20, loss : 0.0759, val_loss : 0.1069, acc : 0.938\n",
            "Epoch 21, loss : 0.0700, val_loss : 0.0713, acc : 1.000\n",
            "Epoch 22, loss : 0.0653, val_loss : 0.0530, acc : 1.000\n",
            "Epoch 23, loss : 0.0614, val_loss : 0.0443, acc : 1.000\n",
            "Epoch 24, loss : 0.0583, val_loss : 0.0419, acc : 1.000\n",
            "Epoch 25, loss : 0.0559, val_loss : 0.0413, acc : 1.000\n",
            "Epoch 26, loss : 0.0540, val_loss : 0.0416, acc : 1.000\n",
            "Epoch 27, loss : 0.0524, val_loss : 0.0417, acc : 1.000\n",
            "Epoch 28, loss : 0.0507, val_loss : 0.0417, acc : 1.000\n",
            "Epoch 29, loss : 0.0492, val_loss : 0.0412, acc : 1.000\n",
            "Epoch 30, loss : 0.0476, val_loss : 0.0403, acc : 1.000\n",
            "Epoch 31, loss : 0.0455, val_loss : 0.0394, acc : 1.000\n",
            "Epoch 32, loss : 0.0425, val_loss : 0.0329, acc : 1.000\n",
            "Epoch 33, loss : 0.0391, val_loss : 0.0293, acc : 1.000\n",
            "Epoch 34, loss : 0.0356, val_loss : 0.0289, acc : 1.000\n",
            "Epoch 35, loss : 0.0319, val_loss : 0.0310, acc : 1.000\n",
            "Epoch 36, loss : 0.0281, val_loss : 0.0372, acc : 1.000\n",
            "Epoch 37, loss : 0.0242, val_loss : 0.0494, acc : 1.000\n",
            "Epoch 38, loss : 0.0213, val_loss : 0.0600, acc : 1.000\n",
            "Epoch 39, loss : 0.0190, val_loss : 0.0637, acc : 1.000\n",
            "Epoch 40, loss : 0.0170, val_loss : 0.0598, acc : 1.000\n",
            "Epoch 41, loss : 0.0154, val_loss : 0.0547, acc : 1.000\n",
            "Epoch 42, loss : 0.0140, val_loss : 0.0473, acc : 1.000\n",
            "Epoch 43, loss : 0.0132, val_loss : 0.0516, acc : 1.000\n",
            "Epoch 44, loss : 0.0123, val_loss : 0.0554, acc : 1.000\n",
            "Epoch 45, loss : 0.0113, val_loss : 0.0501, acc : 1.000\n",
            "Epoch 46, loss : 0.0108, val_loss : 0.0510, acc : 1.000\n",
            "Epoch 47, loss : 0.0104, val_loss : 0.0549, acc : 1.000\n",
            "Epoch 48, loss : 0.0097, val_loss : 0.0518, acc : 1.000\n",
            "Epoch 49, loss : 0.0095, val_loss : 0.0531, acc : 1.000\n",
            "Epoch 50, loss : 0.0091, val_loss : 0.0561, acc : 1.000\n",
            "Epoch 51, loss : 0.0087, val_loss : 0.0541, acc : 1.000\n",
            "Epoch 52, loss : 0.0085, val_loss : 0.0568, acc : 1.000\n",
            "Epoch 53, loss : 0.0082, val_loss : 0.0581, acc : 1.000\n",
            "Epoch 54, loss : 0.0079, val_loss : 0.0567, acc : 1.000\n",
            "Epoch 55, loss : 0.0077, val_loss : 0.0585, acc : 1.000\n",
            "Epoch 56, loss : 0.0074, val_loss : 0.0592, acc : 1.000\n",
            "Epoch 57, loss : 0.0072, val_loss : 0.0592, acc : 0.938\n",
            "Epoch 58, loss : 0.0069, val_loss : 0.0599, acc : 0.938\n",
            "Epoch 59, loss : 0.0067, val_loss : 0.0595, acc : 0.938\n",
            "Epoch 60, loss : 0.0065, val_loss : 0.0614, acc : 0.938\n",
            "Epoch 61, loss : 0.0063, val_loss : 0.0609, acc : 0.938\n",
            "Epoch 62, loss : 0.0061, val_loss : 0.0619, acc : 0.938\n",
            "Epoch 63, loss : 0.0059, val_loss : 0.0614, acc : 0.938\n",
            "Epoch 64, loss : 0.0057, val_loss : 0.0627, acc : 0.938\n",
            "Epoch 65, loss : 0.0055, val_loss : 0.0625, acc : 0.938\n",
            "Epoch 66, loss : 0.0054, val_loss : 0.0632, acc : 0.938\n",
            "Epoch 67, loss : 0.0052, val_loss : 0.0624, acc : 0.938\n",
            "Epoch 68, loss : 0.0051, val_loss : 0.0638, acc : 0.938\n",
            "Epoch 69, loss : 0.0049, val_loss : 0.0629, acc : 0.938\n",
            "Epoch 70, loss : 0.0048, val_loss : 0.0639, acc : 0.938\n",
            "Epoch 71, loss : 0.0046, val_loss : 0.0627, acc : 0.938\n",
            "Epoch 72, loss : 0.0045, val_loss : 0.0639, acc : 0.938\n",
            "Epoch 73, loss : 0.0043, val_loss : 0.0628, acc : 0.938\n",
            "Epoch 74, loss : 0.0042, val_loss : 0.0638, acc : 0.938\n",
            "Epoch 75, loss : 0.0041, val_loss : 0.0634, acc : 0.938\n",
            "Epoch 76, loss : 0.0040, val_loss : 0.0639, acc : 0.938\n",
            "Epoch 77, loss : 0.0039, val_loss : 0.0638, acc : 0.938\n",
            "Epoch 78, loss : 0.0038, val_loss : 0.0639, acc : 0.938\n",
            "Epoch 79, loss : 0.0037, val_loss : 0.0638, acc : 0.938\n",
            "Epoch 80, loss : 0.0036, val_loss : 0.0632, acc : 0.938\n",
            "Epoch 81, loss : 0.0035, val_loss : 0.0637, acc : 0.938\n",
            "Epoch 82, loss : 0.0034, val_loss : 0.0627, acc : 0.938\n",
            "Epoch 83, loss : 0.0034, val_loss : 0.0632, acc : 0.938\n",
            "Epoch 84, loss : 0.0033, val_loss : 0.0623, acc : 0.938\n",
            "Epoch 85, loss : 0.0032, val_loss : 0.0625, acc : 0.938\n",
            "Epoch 86, loss : 0.0031, val_loss : 0.0619, acc : 0.938\n",
            "Epoch 87, loss : 0.0031, val_loss : 0.0619, acc : 0.938\n",
            "Epoch 88, loss : 0.0030, val_loss : 0.0613, acc : 0.938\n",
            "Epoch 89, loss : 0.0029, val_loss : 0.0612, acc : 0.938\n",
            "Epoch 90, loss : 0.0029, val_loss : 0.0607, acc : 0.938\n",
            "Epoch 91, loss : 0.0028, val_loss : 0.0605, acc : 0.938\n",
            "Epoch 92, loss : 0.0028, val_loss : 0.0601, acc : 0.938\n",
            "Epoch 93, loss : 0.0027, val_loss : 0.0599, acc : 0.938\n",
            "Epoch 94, loss : 0.0027, val_loss : 0.0595, acc : 0.938\n",
            "Epoch 95, loss : 0.0026, val_loss : 0.0592, acc : 0.938\n",
            "Epoch 96, loss : 0.0026, val_loss : 0.0588, acc : 0.938\n",
            "Epoch 97, loss : 0.0025, val_loss : 0.0584, acc : 0.938\n",
            "Epoch 98, loss : 0.0025, val_loss : 0.0581, acc : 0.938\n",
            "Epoch 99, loss : 0.0024, val_loss : 0.0577, acc : 0.938\n",
            "test_acc : 0.900\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Binary classification of Iris dataset using neural network implemented in TensorFlow\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.test.gpu_device_name() \n",
        "\"\"\"\n",
        "tensorflowのバージョンを1.x系に変更した際は忘れずに\n",
        "「!pip install tensorflow-gpu==1.14.0」でGPUのインストールをしておきましょう。\n",
        "tf.test.gpu_device_name()でGPUの設定状態を確認し、認識されるかを確認します。\n",
        "成功している場合はログが出力されます、認識されない場合は何も出力されません。\n",
        "\"\"\"\n",
        "\n",
        "#Load dataset\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "\n",
        "#Condition extraction from data frame\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "\n",
        "# NumPy 配列に変換\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# Convert label to number\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "#Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get a mini-batch\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : The following form of ndarray, shape (n_samples, 1)\n",
        "      Correct answer value\n",
        "    batch_size : int\n",
        "      Batch size\n",
        "    seed : int\n",
        "      NumPy random number seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weight and bias\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    return layer_output\n",
        "\n",
        "#Read network structure                              \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "#Initialization of variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "#Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above implementation , weight is initialized by tf.Variable and tf.random_normal ,  AdamOptimizer and tf.nn.sigmoid_cross_entropy_with_logits as optimizer and loss, respectively ."
      ],
      "metadata": {
        "id": "A22I7s9dBLNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a model of Iris using all three types of objective variables"
      ],
      "metadata": {
        "id": "eIHBUmiJCE18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Iris.csv\")"
      ],
      "metadata": {
        "id": "N8jIjqGGG8_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIOTL2aTUaEN",
        "outputId": "a2ac2fbc-26fd-4477-8e3d-db9cf81af4fc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.test.gpu_device_name() \n",
        "\"\"\"\n",
        "tensorflowのバージョンを1.x系に変更した際は忘れずに\n",
        "「!pip install tensorflow-gpu==1.14.0」でGPUのインストールをしておきましょう。\n",
        "tf.test.gpu_device_name()でGPUの設定状態を確認し、認識されるかを確認します。\n",
        "成功している場合はログが出力されます、認識されない場合は何も出力されません。\n",
        "\"\"\"\n",
        "\n",
        "#Load dataset\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "\n",
        "#Condition extraction from data frame\n",
        "# df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "\n",
        "X = df.drop(columns =[\"Species\"])\n",
        "\n",
        "\n",
        "# NumPy 配列に変換\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# Convert label to number\n",
        "y[y == \"Iris-setosa\"] = 0\n",
        "y[y == \"Iris-versicolor\"] = 1\n",
        "y[y == \"Iris-virginica\"] = 2\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "\n",
        "# print(X.shape)\n",
        "\n",
        "#Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train)\n",
        "y_val_one_hot = enc.transform(y_val)\n",
        "y_test_one_hot = enc.transform(y_test)\n",
        "\n",
        "\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get a mini-batch\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : The following form of ndarray, shape (n_samples, 1)\n",
        "      Correct answer value\n",
        "    batch_size : int\n",
        "      Batch size\n",
        "    seed : int\n",
        "      NumPy random number seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3\n",
        "\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weight and bias\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    return layer_output\n",
        "\n",
        "#Read network structure                              \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "#Initialization of variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "#Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Bkm7LrMCDfL",
        "outputId": "137c79bc-a56d-4891-f0aa-a15eacb6e00d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 181.5330, val_loss : 1408.5604, acc : 0.292\n",
            "Epoch 1, loss : 112.5076, val_loss : 754.0248, acc : 0.292\n",
            "Epoch 2, loss : 45.2232, val_loss : 128.4996, acc : 0.292\n",
            "Epoch 3, loss : 13.2326, val_loss : 155.4974, acc : 0.375\n",
            "Epoch 4, loss : 13.2763, val_loss : 121.5533, acc : 0.375\n",
            "Epoch 5, loss : 8.0636, val_loss : 64.8117, acc : 0.375\n",
            "Epoch 6, loss : 5.4852, val_loss : 61.6902, acc : 0.417\n",
            "Epoch 7, loss : 4.2068, val_loss : 43.2824, acc : 0.333\n",
            "Epoch 8, loss : 3.3674, val_loss : 31.5585, acc : 0.417\n",
            "Epoch 9, loss : 2.1315, val_loss : 22.4162, acc : 0.542\n",
            "Epoch 10, loss : 1.2997, val_loss : 18.1738, acc : 0.625\n",
            "Epoch 11, loss : 0.9316, val_loss : 13.3327, acc : 0.667\n",
            "Epoch 12, loss : 0.8794, val_loss : 10.0301, acc : 0.667\n",
            "Epoch 13, loss : 0.5246, val_loss : 10.6943, acc : 0.625\n",
            "Epoch 14, loss : 0.6943, val_loss : 10.1862, acc : 0.750\n",
            "Epoch 15, loss : 0.4131, val_loss : 9.0190, acc : 0.708\n",
            "Epoch 16, loss : 0.4612, val_loss : 7.2907, acc : 0.792\n",
            "Epoch 17, loss : 0.4026, val_loss : 7.5361, acc : 0.708\n",
            "Epoch 18, loss : 0.4468, val_loss : 5.3808, acc : 0.833\n",
            "Epoch 19, loss : 0.2561, val_loss : 5.3732, acc : 0.792\n",
            "Epoch 20, loss : 0.3961, val_loss : 4.7876, acc : 0.875\n",
            "Epoch 21, loss : 0.2181, val_loss : 5.0108, acc : 0.792\n",
            "Epoch 22, loss : 0.4169, val_loss : 6.3038, acc : 0.708\n",
            "Epoch 23, loss : 0.2313, val_loss : 5.2130, acc : 0.708\n",
            "Epoch 24, loss : 0.3175, val_loss : 4.5222, acc : 0.833\n",
            "Epoch 25, loss : 0.3633, val_loss : 4.9092, acc : 0.708\n",
            "Epoch 26, loss : 0.1817, val_loss : 5.2190, acc : 0.750\n",
            "Epoch 27, loss : 0.2591, val_loss : 4.3992, acc : 0.833\n",
            "Epoch 28, loss : 0.2939, val_loss : 5.7762, acc : 0.708\n",
            "Epoch 29, loss : 0.2787, val_loss : 3.9639, acc : 0.833\n",
            "Epoch 30, loss : 0.2437, val_loss : 5.4537, acc : 0.708\n",
            "Epoch 31, loss : 0.2986, val_loss : 3.5249, acc : 0.875\n",
            "Epoch 32, loss : 0.2445, val_loss : 4.4570, acc : 0.708\n",
            "Epoch 33, loss : 0.3109, val_loss : 3.1740, acc : 0.875\n",
            "Epoch 34, loss : 0.2509, val_loss : 2.7165, acc : 0.833\n",
            "Epoch 35, loss : 0.2942, val_loss : 3.2681, acc : 0.750\n",
            "Epoch 36, loss : 0.2922, val_loss : 4.0115, acc : 0.708\n",
            "Epoch 37, loss : 0.3032, val_loss : 4.4192, acc : 0.750\n",
            "Epoch 38, loss : 0.2834, val_loss : 3.9372, acc : 0.750\n",
            "Epoch 39, loss : 0.1240, val_loss : 3.6574, acc : 0.708\n",
            "Epoch 40, loss : 0.2368, val_loss : 2.7101, acc : 0.833\n",
            "Epoch 41, loss : 0.4921, val_loss : 4.3997, acc : 0.833\n",
            "Epoch 42, loss : 0.3516, val_loss : 5.3150, acc : 0.750\n",
            "Epoch 43, loss : 0.2069, val_loss : 1.6048, acc : 0.875\n",
            "Epoch 44, loss : 0.0788, val_loss : 9.9063, acc : 0.625\n",
            "Epoch 45, loss : 0.3311, val_loss : 13.7305, acc : 0.625\n",
            "Epoch 46, loss : 0.2303, val_loss : 16.6517, acc : 0.625\n",
            "Epoch 47, loss : 0.2653, val_loss : 7.7415, acc : 0.625\n",
            "Epoch 48, loss : 0.1430, val_loss : 6.4011, acc : 0.667\n",
            "Epoch 49, loss : 0.3770, val_loss : 3.9604, acc : 0.792\n",
            "Epoch 50, loss : 0.2687, val_loss : 4.6080, acc : 0.708\n",
            "Epoch 51, loss : 0.1146, val_loss : 3.3947, acc : 0.833\n",
            "Epoch 52, loss : 0.2786, val_loss : 3.2005, acc : 0.750\n",
            "Epoch 53, loss : 0.3093, val_loss : 3.5066, acc : 0.833\n",
            "Epoch 54, loss : 0.1379, val_loss : 3.1521, acc : 0.833\n",
            "Epoch 55, loss : 0.3007, val_loss : 3.0108, acc : 0.792\n",
            "Epoch 56, loss : 0.2964, val_loss : 4.5736, acc : 0.750\n",
            "Epoch 57, loss : 0.2706, val_loss : 5.2230, acc : 0.750\n",
            "Epoch 58, loss : 0.1530, val_loss : 2.2151, acc : 0.833\n",
            "Epoch 59, loss : 0.1106, val_loss : 3.6861, acc : 0.750\n",
            "Epoch 60, loss : 0.2289, val_loss : 1.9158, acc : 0.833\n",
            "Epoch 61, loss : 0.1778, val_loss : 2.3083, acc : 0.792\n",
            "Epoch 62, loss : 0.3452, val_loss : 3.9485, acc : 0.792\n",
            "Epoch 63, loss : 0.3078, val_loss : 5.0582, acc : 0.792\n",
            "Epoch 64, loss : 0.0956, val_loss : 2.5163, acc : 0.792\n",
            "Epoch 65, loss : 0.0600, val_loss : 3.6520, acc : 0.750\n",
            "Epoch 66, loss : 0.1813, val_loss : 2.5211, acc : 0.875\n",
            "Epoch 67, loss : 0.5097, val_loss : 7.1785, acc : 0.833\n",
            "Epoch 68, loss : 0.1595, val_loss : 2.9955, acc : 0.708\n",
            "Epoch 69, loss : 0.1180, val_loss : 2.8369, acc : 0.875\n",
            "Epoch 70, loss : 0.2852, val_loss : 3.0790, acc : 0.875\n",
            "Epoch 71, loss : 0.2201, val_loss : 3.0787, acc : 0.708\n",
            "Epoch 72, loss : 0.1403, val_loss : 2.4084, acc : 0.792\n",
            "Epoch 73, loss : 0.1432, val_loss : 2.0493, acc : 0.833\n",
            "Epoch 74, loss : 0.1972, val_loss : 2.1801, acc : 0.792\n",
            "Epoch 75, loss : 0.3737, val_loss : 6.3982, acc : 0.750\n",
            "Epoch 76, loss : 0.2809, val_loss : 3.9712, acc : 0.792\n",
            "Epoch 77, loss : 0.1057, val_loss : 2.0487, acc : 0.917\n",
            "Epoch 78, loss : 0.1045, val_loss : 2.0851, acc : 0.792\n",
            "Epoch 79, loss : 0.0988, val_loss : 2.1001, acc : 0.917\n",
            "Epoch 80, loss : 0.2779, val_loss : 3.2314, acc : 0.875\n",
            "Epoch 81, loss : 0.0696, val_loss : 1.9162, acc : 0.833\n",
            "Epoch 82, loss : 0.0947, val_loss : 2.5491, acc : 0.833\n",
            "Epoch 83, loss : 0.5580, val_loss : 4.9030, acc : 0.875\n",
            "Epoch 84, loss : 0.3434, val_loss : 4.8869, acc : 0.792\n",
            "Epoch 85, loss : 0.0847, val_loss : 1.5089, acc : 0.833\n",
            "Epoch 86, loss : 0.0641, val_loss : 2.1548, acc : 0.875\n",
            "Epoch 87, loss : 0.0983, val_loss : 1.8862, acc : 0.792\n",
            "Epoch 88, loss : 0.3218, val_loss : 4.2236, acc : 0.875\n",
            "Epoch 89, loss : 0.3052, val_loss : 5.9987, acc : 0.833\n",
            "Epoch 90, loss : 0.1008, val_loss : 2.9172, acc : 0.792\n",
            "Epoch 91, loss : 0.0708, val_loss : 1.5299, acc : 0.917\n",
            "Epoch 92, loss : 0.1147, val_loss : 1.1408, acc : 0.875\n",
            "Epoch 93, loss : 0.0803, val_loss : 1.2621, acc : 0.833\n",
            "Epoch 94, loss : 0.1048, val_loss : 2.5548, acc : 0.833\n",
            "Epoch 95, loss : 0.2516, val_loss : 1.0941, acc : 0.833\n",
            "Epoch 96, loss : 0.2209, val_loss : 1.5367, acc : 0.875\n",
            "Epoch 97, loss : 0.3530, val_loss : 6.8941, acc : 0.792\n",
            "Epoch 98, loss : 0.1651, val_loss : 1.1935, acc : 0.792\n",
            "Epoch 99, loss : 0.0429, val_loss : 6.0071, acc : 0.792\n",
            "test_acc : 0.933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a model of House Prices"
      ],
      "metadata": {
        "id": "O66KC2XFfA-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"train.csv\")"
      ],
      "metadata": {
        "id": "OqHnWEMETz9w"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.io.pytables import DataCol\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.test.gpu_device_name() \n",
        "\"\"\"\n",
        "tensorflowのバージョンを1.x系に変更した際は忘れずに\n",
        "「!pip install tensorflow-gpu==1.14.0」でGPUのインストールをしておきましょう。\n",
        "tf.test.gpu_device_name()でGPUの設定状態を確認し、認識されるかを確認します。\n",
        "成功している場合はログが出力されます、認識されない場合は何も出力されません。\n",
        "\"\"\"\n",
        "\n",
        "#Load dataset\n",
        "df = data\n",
        "\n",
        "#Condition extraction from data frame\n",
        "# df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"SalePrice\"]\n",
        "\n",
        "X = df[[\"GrLivArea\",\"YearBuilt\"]]\n",
        "\n",
        "\n",
        "# NumPy 配列に変換\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# Convert label to number\n",
        "\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "\n",
        "# print(X.shape)\n",
        "\n",
        "#Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# from sklearn.preprocessing import OneHotEncoder\n",
        "# enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "# y_train_one_hot = enc.fit_transform(y_train)\n",
        "# y_val_one_hot = enc.transform(y_val)\n",
        "# y_test_one_hot = enc.transform(y_test)\n",
        "\n",
        "\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get a mini-batch\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : The following form of ndarray, shape (n_samples, 1)\n",
        "      Correct answer value\n",
        "    batch_size : int\n",
        "      Batch size\n",
        "    seed : int\n",
        "      NumPy random number seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weight and bias\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    return layer_output\n",
        "\n",
        "#Read network structure                              \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op =tf.losses.mean_squared_error(Y,logits)\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimated result\n",
        "pred = logits\n",
        "\n",
        "\n",
        "\n",
        "#Initialization of variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "#Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss= sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, total_loss, val_loss))\n",
        "    test_loss = sess.run(loss_op, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_loss : {:.3f}\".format(test_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgruUYTBlKiA",
        "outputId": "1b2b95cb-1b18-4ce5-a2b3-2d77a36df4f2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 5364992280.6681, val_loss : 13801254912.0000\n",
            "Epoch 1, loss : 675773047.5717, val_loss : 3365656576.0000\n",
            "Epoch 2, loss : 388315487.9315, val_loss : 2918411520.0000\n",
            "Epoch 3, loss : 349904952.9422, val_loss : 2628573440.0000\n",
            "Epoch 4, loss : 326289470.3555, val_loss : 2506779392.0000\n",
            "Epoch 5, loss : 315542327.0236, val_loss : 2463290880.0000\n",
            "Epoch 6, loss : 310899263.5546, val_loss : 2452172288.0000\n",
            "Epoch 7, loss : 308968539.7173, val_loss : 2451699968.0000\n",
            "Epoch 8, loss : 308144278.3041, val_loss : 2453661184.0000\n",
            "Epoch 9, loss : 307746079.4176, val_loss : 2456043776.0000\n",
            "Epoch 10, loss : 307571882.1071, val_loss : 2458191872.0000\n",
            "Epoch 11, loss : 307497967.8630, val_loss : 2459581440.0000\n",
            "Epoch 12, loss : 307472080.4797, val_loss : 2460525568.0000\n",
            "Epoch 13, loss : 307411116.9850, val_loss : 2461540864.0000\n",
            "Epoch 14, loss : 307438070.9208, val_loss : 2462265344.0000\n",
            "Epoch 15, loss : 307433477.3105, val_loss : 2463004416.0000\n",
            "Epoch 16, loss : 307448500.9336, val_loss : 2463984384.0000\n",
            "Epoch 17, loss : 307479717.6874, val_loss : 2465359104.0000\n",
            "Epoch 18, loss : 307493037.2934, val_loss : 2466347264.0000\n",
            "Epoch 19, loss : 307507449.3533, val_loss : 2467081984.0000\n",
            "Epoch 20, loss : 307548697.5931, val_loss : 2468642816.0000\n",
            "Epoch 21, loss : 307593369.6959, val_loss : 2469707520.0000\n",
            "Epoch 22, loss : 307612182.5439, val_loss : 2469825280.0000\n",
            "Epoch 23, loss : 307615150.9722, val_loss : 2470263808.0000\n",
            "Epoch 24, loss : 307615038.0814, val_loss : 2471469824.0000\n",
            "Epoch 25, loss : 307694860.8822, val_loss : 2472421120.0000\n",
            "Epoch 26, loss : 307748139.9572, val_loss : 2473094144.0000\n",
            "Epoch 27, loss : 307744005.9615, val_loss : 2473919744.0000\n",
            "Epoch 28, loss : 307778748.0257, val_loss : 2473940992.0000\n",
            "Epoch 29, loss : 307757186.8094, val_loss : 2474400512.0000\n",
            "Epoch 30, loss : 307804539.3405, val_loss : 2475836416.0000\n",
            "Epoch 31, loss : 307847611.8887, val_loss : 2476618240.0000\n",
            "Epoch 32, loss : 307854602.5182, val_loss : 2476709888.0000\n",
            "Epoch 33, loss : 307844562.2955, val_loss : 2476988416.0000\n",
            "Epoch 34, loss : 307824561.1991, val_loss : 2477871872.0000\n",
            "Epoch 35, loss : 307860912.7537, val_loss : 2477632256.0000\n",
            "Epoch 36, loss : 307880397.3619, val_loss : 2477959168.0000\n",
            "Epoch 37, loss : 307870055.1263, val_loss : 2478321408.0000\n",
            "Epoch 38, loss : 307841254.9550, val_loss : 2477877760.0000\n",
            "Epoch 39, loss : 307860802.5696, val_loss : 2478443008.0000\n",
            "Epoch 40, loss : 307873390.7323, val_loss : 2478338560.0000\n",
            "Epoch 41, loss : 307847223.6745, val_loss : 2478861568.0000\n",
            "Epoch 42, loss : 307906127.7944, val_loss : 2478860544.0000\n",
            "Epoch 43, loss : 307867627.6831, val_loss : 2479262976.0000\n",
            "Epoch 44, loss : 307905540.8994, val_loss : 2479145472.0000\n",
            "Epoch 45, loss : 307876874.1071, val_loss : 2479920384.0000\n",
            "Epoch 46, loss : 307928576.3084, val_loss : 2479217408.0000\n",
            "Epoch 47, loss : 307880887.4690, val_loss : 2479644672.0000\n",
            "Epoch 48, loss : 307908909.1906, val_loss : 2479921408.0000\n",
            "Epoch 49, loss : 307874524.5396, val_loss : 2479003136.0000\n",
            "Epoch 50, loss : 307889182.9036, val_loss : 2479211520.0000\n",
            "Epoch 51, loss : 307892911.3148, val_loss : 2479610880.0000\n",
            "Epoch 52, loss : 307854681.1820, val_loss : 2479068160.0000\n",
            "Epoch 53, loss : 307867897.7987, val_loss : 2479372800.0000\n",
            "Epoch 54, loss : 307849405.7730, val_loss : 2479151104.0000\n",
            "Epoch 55, loss : 307859175.7773, val_loss : 2478676224.0000\n",
            "Epoch 56, loss : 307805558.5096, val_loss : 2478600192.0000\n",
            "Epoch 57, loss : 307827912.4625, val_loss : 2478807040.0000\n",
            "Epoch 58, loss : 307824817.7473, val_loss : 2478812672.0000\n",
            "Epoch 59, loss : 307834053.6874, val_loss : 2477620992.0000\n",
            "Epoch 60, loss : 307769228.6424, val_loss : 2478466560.0000\n",
            "Epoch 61, loss : 307801987.0493, val_loss : 2477812736.0000\n",
            "Epoch 62, loss : 307772839.7773, val_loss : 2478129664.0000\n",
            "Epoch 63, loss : 307761296.0343, val_loss : 2478215936.0000\n",
            "Epoch 64, loss : 307787755.4090, val_loss : 2477659136.0000\n",
            "Epoch 65, loss : 307748614.5439, val_loss : 2477661952.0000\n",
            "Epoch 66, loss : 307733126.3383, val_loss : 2477408000.0000\n",
            "Epoch 67, loss : 307729200.5482, val_loss : 2477961472.0000\n",
            "Epoch 68, loss : 307740877.1221, val_loss : 2477518848.0000\n",
            "Epoch 69, loss : 307728641.9529, val_loss : 2477711104.0000\n",
            "Epoch 70, loss : 307740903.4690, val_loss : 2477803520.0000\n",
            "Epoch 71, loss : 307686798.0471, val_loss : 2476580608.0000\n",
            "Epoch 72, loss : 307689660.0942, val_loss : 2477103360.0000\n",
            "Epoch 73, loss : 307698428.6424, val_loss : 2476874752.0000\n",
            "Epoch 74, loss : 307658424.3597, val_loss : 2477092864.0000\n",
            "Epoch 75, loss : 307720651.6831, val_loss : 2476560384.0000\n",
            "Epoch 76, loss : 307659489.6103, val_loss : 2476330752.0000\n",
            "Epoch 77, loss : 307674824.2227, val_loss : 2476922368.0000\n",
            "Epoch 78, loss : 307693785.0107, val_loss : 2477264640.0000\n",
            "Epoch 79, loss : 307697653.4475, val_loss : 2476007424.0000\n",
            "Epoch 80, loss : 307618840.9422, val_loss : 2475968768.0000\n",
            "Epoch 81, loss : 307662278.0642, val_loss : 2476471296.0000\n",
            "Epoch 82, loss : 307660828.1627, val_loss : 2475479296.0000\n",
            "Epoch 83, loss : 307625853.9443, val_loss : 2476239616.0000\n",
            "Epoch 84, loss : 307632958.7666, val_loss : 2475464448.0000\n",
            "Epoch 85, loss : 307628771.7687, val_loss : 2475672832.0000\n",
            "Epoch 86, loss : 307610741.1392, val_loss : 2475987968.0000\n",
            "Epoch 87, loss : 307620369.3362, val_loss : 2475576832.0000\n",
            "Epoch 88, loss : 307629303.0236, val_loss : 2474995200.0000\n",
            "Epoch 89, loss : 307593416.9079, val_loss : 2475654912.0000\n",
            "Epoch 90, loss : 307602010.5182, val_loss : 2474351360.0000\n",
            "Epoch 91, loss : 307560702.2184, val_loss : 2475244032.0000\n",
            "Epoch 92, loss : 307603561.4218, val_loss : 2474574336.0000\n",
            "Epoch 93, loss : 307570103.1606, val_loss : 2474157056.0000\n",
            "Epoch 94, loss : 307561630.8351, val_loss : 2474991616.0000\n",
            "Epoch 95, loss : 307587065.0107, val_loss : 2474340352.0000\n",
            "Epoch 96, loss : 307553173.7901, val_loss : 2474223616.0000\n",
            "Epoch 97, loss : 307559470.0471, val_loss : 2474451968.0000\n",
            "Epoch 98, loss : 307552251.8887, val_loss : 2474099456.0000\n",
            "Epoch 99, loss : 307563031.7773, val_loss : 2474729216.0000\n",
            "test_loss : 3838956032.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a MNIST model"
      ],
      "metadata": {
        "id": "fecp27c-rfIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtcJS6w-qARr",
        "outputId": "2af20091-5639-47ff-96b5-250cb5aaf82e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)"
      ],
      "metadata": {
        "id": "M6QHj5VtsKMA"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n"
      ],
      "metadata": {
        "id": "brnxSadMsN-p"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n"
      ],
      "metadata": {
        "id": "FZShzU8hsQCQ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_new, X_val, y_train_new, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
      ],
      "metadata": {
        "id": "PpNi85ycsRx4"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get a mini-batch\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : The following form of ndarray, shape (n_samples, 1)\n",
        "      Correct answer value\n",
        "    batch_size : int\n",
        "      Batch size\n",
        "    seed : int\n",
        "      NumPy random number seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Hyperparameter settings\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "\n",
        "#Determine the shape of the argument to be passed to the calculation graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# train mini batch iterator\n",
        "get_mini_batch_train = GetMiniBatch(X_train_new, y_train_new, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    Simple 3-layer neural network\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weight and bias\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.add and + are equivalent\n",
        "    return layer_output\n",
        "\n",
        "#Read network structure                              \n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Estimated result\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
        "#Indicator value calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "#Initialization of variable\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "#Run calculation graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        #Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soWsSWWWsTuf",
        "outputId": "3618db31-bff2-4c2a-99ef-bf675582e765"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 2.4055, val_loss : 8.2915, acc : 0.793\n",
            "Epoch 1, loss : 0.3962, val_loss : 3.6295, acc : 0.806\n",
            "Epoch 2, loss : 0.1719, val_loss : 1.8102, acc : 0.789\n",
            "Epoch 3, loss : 0.0893, val_loss : 1.1681, acc : 0.800\n",
            "Epoch 4, loss : 0.0616, val_loss : 0.9340, acc : 0.822\n",
            "Epoch 5, loss : 0.0483, val_loss : 0.8642, acc : 0.843\n",
            "Epoch 6, loss : 0.0409, val_loss : 0.7832, acc : 0.860\n",
            "Epoch 7, loss : 0.0350, val_loss : 0.7052, acc : 0.875\n",
            "Epoch 8, loss : 0.0312, val_loss : 0.6504, acc : 0.885\n",
            "Epoch 9, loss : 0.0280, val_loss : 0.6312, acc : 0.886\n",
            "Epoch 10, loss : 0.0250, val_loss : 0.6662, acc : 0.895\n",
            "Epoch 11, loss : 0.0233, val_loss : 0.6201, acc : 0.894\n",
            "Epoch 12, loss : 0.0219, val_loss : 0.6008, acc : 0.904\n",
            "Epoch 13, loss : 0.0205, val_loss : 0.6137, acc : 0.908\n",
            "Epoch 14, loss : 0.0197, val_loss : 0.6218, acc : 0.905\n",
            "Epoch 15, loss : 0.0183, val_loss : 0.5971, acc : 0.910\n",
            "Epoch 16, loss : 0.0176, val_loss : 0.6583, acc : 0.909\n",
            "Epoch 17, loss : 0.0166, val_loss : 0.6255, acc : 0.913\n",
            "Epoch 18, loss : 0.0165, val_loss : 0.6239, acc : 0.910\n",
            "Epoch 19, loss : 0.0152, val_loss : 0.6087, acc : 0.917\n",
            "Epoch 20, loss : 0.0146, val_loss : 0.6202, acc : 0.920\n",
            "Epoch 21, loss : 0.0142, val_loss : 0.6068, acc : 0.917\n",
            "Epoch 22, loss : 0.0138, val_loss : 0.6523, acc : 0.917\n",
            "Epoch 23, loss : 0.0130, val_loss : 0.6161, acc : 0.923\n",
            "Epoch 24, loss : 0.0129, val_loss : 0.6159, acc : 0.921\n",
            "Epoch 25, loss : 0.0121, val_loss : 0.6103, acc : 0.924\n",
            "Epoch 26, loss : 0.0117, val_loss : 0.6194, acc : 0.922\n",
            "Epoch 27, loss : 0.0114, val_loss : 0.6120, acc : 0.923\n",
            "Epoch 28, loss : 0.0111, val_loss : 0.6482, acc : 0.921\n",
            "Epoch 29, loss : 0.0108, val_loss : 0.6273, acc : 0.928\n",
            "Epoch 30, loss : 0.0105, val_loss : 0.6397, acc : 0.928\n",
            "Epoch 31, loss : 0.0101, val_loss : 0.6391, acc : 0.930\n",
            "Epoch 32, loss : 0.0099, val_loss : 0.6468, acc : 0.928\n",
            "Epoch 33, loss : 0.0095, val_loss : 0.6324, acc : 0.927\n",
            "Epoch 34, loss : 0.0094, val_loss : 0.6453, acc : 0.926\n",
            "Epoch 35, loss : 0.0090, val_loss : 0.6369, acc : 0.928\n",
            "Epoch 36, loss : 0.0088, val_loss : 0.6439, acc : 0.931\n",
            "Epoch 37, loss : 0.0084, val_loss : 0.6450, acc : 0.930\n",
            "Epoch 38, loss : 0.0085, val_loss : 0.6762, acc : 0.928\n",
            "Epoch 39, loss : 0.0081, val_loss : 0.6813, acc : 0.928\n",
            "Epoch 40, loss : 0.0081, val_loss : 0.6581, acc : 0.933\n",
            "Epoch 41, loss : 0.0078, val_loss : 0.6934, acc : 0.933\n",
            "Epoch 42, loss : 0.0077, val_loss : 0.6627, acc : 0.931\n",
            "Epoch 43, loss : 0.0075, val_loss : 0.6936, acc : 0.934\n",
            "Epoch 44, loss : 0.0073, val_loss : 0.7333, acc : 0.931\n",
            "Epoch 45, loss : 0.0073, val_loss : 0.7128, acc : 0.934\n",
            "Epoch 46, loss : 0.0070, val_loss : 0.7180, acc : 0.932\n",
            "Epoch 47, loss : 0.0069, val_loss : 0.6976, acc : 0.933\n",
            "Epoch 48, loss : 0.0070, val_loss : 0.7433, acc : 0.934\n",
            "Epoch 49, loss : 0.0064, val_loss : 0.6903, acc : 0.936\n",
            "Epoch 50, loss : 0.0063, val_loss : 0.6946, acc : 0.937\n",
            "Epoch 51, loss : 0.0065, val_loss : 0.6965, acc : 0.935\n",
            "Epoch 52, loss : 0.0061, val_loss : 0.7258, acc : 0.934\n",
            "Epoch 53, loss : 0.0060, val_loss : 0.7457, acc : 0.932\n",
            "Epoch 54, loss : 0.0059, val_loss : 0.7285, acc : 0.936\n",
            "Epoch 55, loss : 0.0060, val_loss : 0.7198, acc : 0.935\n",
            "Epoch 56, loss : 0.0057, val_loss : 0.7689, acc : 0.934\n",
            "Epoch 57, loss : 0.0055, val_loss : 0.7251, acc : 0.936\n",
            "Epoch 58, loss : 0.0055, val_loss : 0.7515, acc : 0.937\n",
            "Epoch 59, loss : 0.0055, val_loss : 0.7913, acc : 0.938\n",
            "Epoch 60, loss : 0.0054, val_loss : 0.7537, acc : 0.939\n",
            "Epoch 61, loss : 0.0053, val_loss : 0.8007, acc : 0.936\n",
            "Epoch 62, loss : 0.0050, val_loss : 0.7400, acc : 0.936\n",
            "Epoch 63, loss : 0.0050, val_loss : 0.8014, acc : 0.936\n",
            "Epoch 64, loss : 0.0048, val_loss : 0.7927, acc : 0.938\n",
            "Epoch 65, loss : 0.0049, val_loss : 0.8106, acc : 0.939\n",
            "Epoch 66, loss : 0.0047, val_loss : 0.8047, acc : 0.936\n",
            "Epoch 67, loss : 0.0047, val_loss : 0.8264, acc : 0.937\n",
            "Epoch 68, loss : 0.0045, val_loss : 0.8747, acc : 0.935\n",
            "Epoch 69, loss : 0.0045, val_loss : 0.8347, acc : 0.938\n",
            "Epoch 70, loss : 0.0045, val_loss : 0.8470, acc : 0.936\n",
            "Epoch 71, loss : 0.0043, val_loss : 0.8768, acc : 0.939\n",
            "Epoch 72, loss : 0.0043, val_loss : 0.8551, acc : 0.937\n",
            "Epoch 73, loss : 0.0042, val_loss : 0.9010, acc : 0.937\n",
            "Epoch 74, loss : 0.0045, val_loss : 0.8317, acc : 0.941\n",
            "Epoch 75, loss : 0.0042, val_loss : 0.8215, acc : 0.942\n",
            "Epoch 76, loss : 0.0041, val_loss : 0.8397, acc : 0.940\n",
            "Epoch 77, loss : 0.0042, val_loss : 0.8604, acc : 0.942\n",
            "Epoch 78, loss : 0.0039, val_loss : 0.8852, acc : 0.939\n",
            "Epoch 79, loss : 0.0040, val_loss : 0.8418, acc : 0.940\n",
            "Epoch 80, loss : 0.0039, val_loss : 0.8340, acc : 0.942\n",
            "Epoch 81, loss : 0.0039, val_loss : 0.8565, acc : 0.942\n",
            "Epoch 82, loss : 0.0038, val_loss : 0.8747, acc : 0.942\n",
            "Epoch 83, loss : 0.0040, val_loss : 0.8696, acc : 0.939\n",
            "Epoch 84, loss : 0.0037, val_loss : 0.8941, acc : 0.943\n",
            "Epoch 85, loss : 0.0037, val_loss : 0.8539, acc : 0.941\n",
            "Epoch 86, loss : 0.0037, val_loss : 0.9590, acc : 0.940\n",
            "Epoch 87, loss : 0.0036, val_loss : 0.9094, acc : 0.941\n",
            "Epoch 88, loss : 0.0037, val_loss : 0.8825, acc : 0.944\n",
            "Epoch 89, loss : 0.0035, val_loss : 0.9407, acc : 0.940\n",
            "Epoch 90, loss : 0.0035, val_loss : 0.9732, acc : 0.940\n",
            "Epoch 91, loss : 0.0036, val_loss : 0.9035, acc : 0.943\n",
            "Epoch 92, loss : 0.0033, val_loss : 0.9252, acc : 0.942\n",
            "Epoch 93, loss : 0.0034, val_loss : 0.9551, acc : 0.942\n",
            "Epoch 94, loss : 0.0033, val_loss : 0.9280, acc : 0.943\n",
            "Epoch 95, loss : 0.0032, val_loss : 0.9309, acc : 0.944\n",
            "Epoch 96, loss : 0.0034, val_loss : 0.9380, acc : 0.941\n",
            "Epoch 97, loss : 0.0032, val_loss : 0.9310, acc : 0.941\n",
            "Epoch 98, loss : 0.0032, val_loss : 0.9335, acc : 0.943\n",
            "Epoch 99, loss : 0.0031, val_loss : 0.9591, acc : 0.942\n",
            "test_acc : 0.944\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
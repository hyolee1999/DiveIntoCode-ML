{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a one-dimensional convolutional layer class that limits the number of channels to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d():\n",
    "\n",
    "    def forward(self, x, w, b):\n",
    "        lst = []\n",
    "        for i in range(len(w) - 1):\n",
    "            lst.append((x[i:i+len(w)] * w).sum() + b[0] )\n",
    "        return np.array(lst)\n",
    "    \n",
    "    def backward(self,x,w,da):\n",
    "        dw = np.array([da @ x[i:i+len(da)] for i in range(len(w))])\n",
    "        db = np.sum(da)\n",
    "        dx = []\n",
    "        new_w = np.insert(w[::-1], 0, 0)\n",
    "        new_w = np.append(new_w, 0)\n",
    "        for i in range(len(new_w)-1):\n",
    "            dx.append(new_w[i:i+len(da)] @ da)\n",
    "        dx = np.array(dx[::-1])\n",
    "        return db, dw, dx\n",
    "        \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Output size calculation after one-dimensional convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_size_calculation(n_in, P, F, S):\n",
    "    n_out = int((n_in + 2*P - F) / S + 1)\n",
    "    return n_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment of one-dimensional convolutional layer with small array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "delta_a = np.array([10, 20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35 50]\n",
      "30\n",
      "[ 50  80 110]\n",
      "[ 30 110 170 140]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "conv = SimpleConv1d()\n",
    "# db, dw, dx = SC1D.backward(x, w, da)\n",
    "print(conv.forward(x, w, b))\n",
    "db, dw, dx = conv.backward(x, w, delta_a)\n",
    "print(db)\n",
    "print(dw)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a one-dimensional convolutional layer class that does not limit the number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    Simple initialization with Gaussian distribution\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self, n_nodes1, n_nodes2,kernel_size):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          Number of nodes in the previous layer\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma *np.random.randn(n_nodes1,n_nodes2,kernel_size)\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma *np.random.randn(n_nodes2)\n",
    "        return B\n",
    "\n",
    "class Conv1d():\n",
    "    def __init__(self, input_channels,output_channels, features, initializer,kernel_size):\n",
    "#         self.optimizer = optimizer\n",
    "        self.kernel_size = kernel_size\n",
    "        # Initialize\n",
    "        # Initialize self.W and self.B using the initializer method\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.features = features\n",
    "        self.W = initializer.W(output_channels,self.input_channels,kernel_size)\n",
    "        self.B = initializer.B(output_channels)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        lst = []\n",
    "#         X = np.pad(X, ((0,0), ((self.kernel_size-1), 0)))\n",
    "\n",
    "        new_X = np.tile(X,(self.output_channels,1))\n",
    "        self.new_X = new_X.reshape(-1,X.shape[0],X.shape[1])\n",
    "\n",
    "        for i in range(self.new_X.shape[-1] - self.kernel_size + 1):\n",
    "            lst.append((self.new_X[:,:,i:i+self.W.shape[-1]] * self.W).sum(axis = (1,2)) + self.B )\n",
    "        return np.array(lst).T\n",
    "    \n",
    "    def backward(self,dA):\n",
    "        new_dA = np.tile(np.expand_dims(dA,axis = 1),(1,self.input_channels,1))\n",
    "        dW = np.sum(np.array([new_dA * self.new_X[:,:,i:i+dA.shape[-1]] for i in range(self.W.shape[-1])]),axis = -1).T.reshape(self.W.shape)\n",
    "        dB = np.sum(dA,axis = 1)\n",
    "        new_w = np.concatenate((self.W,np.zeros((self.W.shape[0],self.W.shape[1],self.kernel_size-2))),axis = -1)\n",
    "        new_w = np.concatenate((np.zeros((self.W.shape[0],self.W.shape[1],self.kernel_size-2)),new_w),axis = -1)\n",
    "        dx = []\n",
    "        for i in range(new_w.shape[-1]- dA.shape[-1] + 1):\n",
    "            dx.append(np.sum(new_w[:,:,i:i+dA.shape[-1]] * new_dA,axis = (0,2)))\n",
    "        return np.array(dx[::-1]).T\n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) #shape (2, 4), (number of input channels, number of features).\n",
    "w = np.ones((3, 2, 3)) # Set to 1 for simplification of the example. (Number of output channels, number of input channels, filter size).\n",
    "b = np.array([1, 2, 3]) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv1d(2,3,4,SimpleInitializer(0.2),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W = w\n",
    "model.B = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 22.],\n",
       "       [17., 23.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing = model.forward(x)\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 69., 120., 120.,  51.],\n",
       "       [ 69., 120., 120.,  51.]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.backward(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d():\n",
    "    def __init__(self, input_channels,output_channels, features, initializer,kernel_size,padding):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        # Initialize\n",
    "        # Initialize self.W and self.B using the initializer method\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.features = features\n",
    "        self.W = initializer.W(output_channels,self.input_channels,kernel_size)\n",
    "        self.B = initializer.B(output_channels)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        lst = []\n",
    "        X = np.pad(X, ((0,0), (self.padding, self.padding)))\n",
    "        self.X =X\n",
    "        new_X = np.tile(X,(self.output_channels,1))\n",
    "        self.new_X = new_X.reshape(-1,X.shape[0],X.shape[1])\n",
    "        for i in range(self.new_X .shape[-1] - self.kernel_size + 1):\n",
    "            lst.append((self.new_X[:,:,i:i+self.W.shape[-1]] * self.W).sum(axis = (1,2)) + self.B )\n",
    "        return np.array(lst).T\n",
    "    \n",
    "    def backward(self,dA):\n",
    "        new_dA = np.tile(np.expand_dims(dA,axis = 1),(1,self.input_channels,1))\n",
    "        dW = np.sum(np.array([new_dA * self.new_X[:,:,i:i+dA.shape[-1]] for i in range(self.W.shape[-1])]),axis = -1).T.reshape(self.W.shape)\n",
    "    \n",
    "        dB = np.sum(dA,axis = 1)\n",
    "        w_shape = int((dA.shape[-1] - 1 + self.X.shape[-1] - self.kernel_size)/2)\n",
    "        new_w = np.concatenate((self.W,np.zeros((self.W.shape[0],self.W.shape[1],w_shape))),axis = -1)\n",
    "        new_w = np.concatenate((np.zeros((self.W.shape[0],self.W.shape[1],w_shape)),new_w),axis = -1)\n",
    "        dx = []\n",
    "\n",
    "        for i in range(new_w.shape[-1]- dA.shape[-1] + 1):\n",
    "            dx.append(np.sum(new_w[:,:,i:i+new_dA.shape[-1]] * new_dA,axis = (0,2)))\n",
    "\n",
    "        return np.array(dx[::-1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv1d(2,3,4,SimpleInitializer(0.2),3,padding = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W = w\n",
    "model.B = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = model.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  9., 16., 22., 17., 10.,  1.],\n",
       "       [ 2.,  5., 10., 17., 23., 18., 11.,  2.],\n",
       "       [ 3.,  6., 11., 18., 24., 19., 12.,  3.]])"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.,  21.,  51.,  96., 150., 174., 156.,  93.,  39.,   6.],\n",
       "       [  6.,  21.,  51.,  96., 150., 174., 156.,  93.,  39.,   6.]])"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.backward(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response to mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1] \n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d():\n",
    "    def __init__(self,input_channels,output_channels, features, initializer,kernel_size,padding):\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        # Initialize\n",
    "        # Initialize self.W and self.B using the initializer method\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.features = features\n",
    "        self.W = initializer.W(output_channels,self.input_channels,kernel_size)\n",
    "        self.B = initializer.B(output_channels)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        lst = []\n",
    "        X = np.pad(X, ((0,0),(0,0), (self.padding, self.padding)))\n",
    "        self.X =X\n",
    "        new_X = np.tile(X,(1,self.output_channels,1))\n",
    "        self.new_X = new_X.reshape(X.shape[0],-1,X.shape[1],X.shape[2])\n",
    "     \n",
    "        for i in range(self.new_X.shape[-1] - self.kernel_size + 1):\n",
    "            lst.append((self.new_X[:,:,:,i:i+self.W.shape[-1]] * np.expand_dims(self.W,axis = 0)).sum(axis = (2,3)) + self.B )\n",
    "#         return np.concatenate(lst,axis = 1)\n",
    "        return np.array(lst).T.transpose((1,0,2))\n",
    "    \n",
    "    def backward(self,dA):\n",
    "        new_dA = np.tile(np.expand_dims(dA,axis = 2),(1,1,self.input_channels,1))\n",
    "        dW = np.sum(np.array([new_dA * self.new_X[:,:,:,i:i+dA.shape[-1]] for i in range(self.W.shape[-1])]),axis = -1).transpose((1,2,3,0))\n",
    "        dW = np.sum(dW,axis = 0)\n",
    "\n",
    "#         dW = np.sum(np.array([new_dA * self.new_X[:,:,:,i:i+dA.shape[-1]] for i in range(self.W.shape[-1])]),axis = (0,-1)).T.reshape(self.W.shape)\n",
    "    \n",
    "        dB = np.sum(dA,axis = (0,-1))\n",
    "        w_shape = int((dA.shape[-1] - 1 + self.X.shape[-1] - self.kernel_size)/2)\n",
    "        new_w = np.concatenate((self.W,np.zeros((self.W.shape[0],self.W.shape[1],w_shape))),axis = -1)\n",
    "        new_w = np.concatenate((np.zeros((self.W.shape[0],self.W.shape[1],w_shape)),new_w),axis = -1)\n",
    "        new_w = np.expand_dims(new_w,axis = 0)\n",
    "        dx = []\n",
    "\n",
    "        for i in range(new_w.shape[-1]- dA.shape[-1] + 1):\n",
    "            dx.append(np.sum(new_w[:,:,:,i:i+new_dA.shape[-1]] * new_dA,axis = (1,3)))\n",
    "\n",
    "        return np.array(dx)[::-1].transpose((1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv1d(2,3,4,SimpleInitializer(0.2),3,padding = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W = w\n",
    "model.B = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "test = np.random.rand(3,2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = model.forward(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 6)"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 8.91740491, 21.91806026, 38.03976737, 46.63663321,\n",
       "         47.06699818, 41.25525962, 23.74098886, 10.30996855],\n",
       "        [ 8.91740491, 21.91806026, 38.03976737, 46.63663321,\n",
       "         47.06699818, 41.25525962, 23.74098886, 10.30996855]],\n",
       "\n",
       "       [[10.59512196, 25.1173584 , 42.22787813, 45.99622658,\n",
       "         41.91034609, 32.64789902, 18.28442861,  7.84807266],\n",
       "        [10.59512196, 25.1173584 , 42.22787813, 45.99622658,\n",
       "         41.91034609, 32.64789902, 18.28442861,  7.84807266]],\n",
       "\n",
       "       [[ 8.99651022, 22.88835567, 40.49910946, 51.06846678,\n",
       "         51.84715364, 45.18802383, 25.62215629, 10.95162397],\n",
       "        [ 8.99651022, 22.88835567, 40.49910946, 51.06846678,\n",
       "         51.84715364, 45.18802383, 25.62215629, 10.95162397]]])"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.backward(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbitrary number of strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d():\n",
    "    def __init__(self,input_channels,output_channels, initializer,kernel_size,optimizer,padding = 0,stride = 1):\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.optimizer = optimizer\n",
    "        # Initialize\n",
    "        # Initialize self.W and self.B using the initializer method\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        \n",
    "        self.W = initializer.W(output_channels,input_channels,kernel_size)\n",
    "        self.B = initializer.B(output_channels)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_in = X.shape[-1]\n",
    "\n",
    "        X = X.reshape(self.n_samples, self.input_channels, self.n_in)\n",
    "        self.X = np.pad(X, ((0,0), (0,0), ((self.kernel_size-1), 0)))\n",
    "        self.X1 = np.zeros((self.n_samples, self.input_channels, self.kernel_size, self.n_in+(self.kernel_size-1)))\n",
    "        for i in range(self.kernel_size):\n",
    "            self.X1[:, :, i] = np.roll(self.X, -i, axis=-1)\n",
    "        A = np.sum(self.X1[:, np.newaxis, :, :, self.kernel_size-1-self.padding:self.n_in+self.padding:self.stride]*self.W[:, :, :, np.newaxis], axis=(2, 3)) + self.B.reshape(-1,1)\n",
    "        return A\n",
    "    \n",
    "    def backward(self,dA):\n",
    "        self.dW = np.sum(dA[:, :, np.newaxis, np.newaxis]*self.X1[:, np.newaxis, :, :, self.kernel_size-1-self.padding:self.n_in+self.padding:self.stride], axis=(0, -1))\n",
    "        self.dB = np.sum(dA, axis=(0, -1))\n",
    "        self.dA = np.pad(dA, ((0,0), (0,0), (0, (self.kernel_size-1))))\n",
    "        self.dA1 = np.zeros((self.n_samples, self.output_channels, self.kernel_size, self.dA.shape[-1]))\n",
    "        for i in range(self.kernel_size):\n",
    "            self.dA1[:, :, i] = np.roll(self.dA, i, axis=-1)\n",
    "        dX = np.sum(self.W[:, :, :, np.newaxis]*self.dA1[:, :, np.newaxis], axis=(1,3))\n",
    "        self.optimizer.update(self)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning and estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_new, X_val, y_train_new, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_mini_batch = GetMiniBatch(X_train_new, y_train_new, batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    Simple initialization with Gaussian distribution\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Standard deviation of Gaussian distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    def W(self,  n_nodes1, n_nodes2,kernel_size):\n",
    "        \"\"\"\n",
    "        Weight initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          Number of nodes in the previous layer\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma *np.random.randn(n_nodes1, n_nodes2,kernel_size)\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        Bias initialization\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          Number of nodes in the later layer\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma *np.random.randn(n_nodes2)\n",
    "        return B\n",
    "    \n",
    "class XavierInitializer:\n",
    "    def __init__(self,sigma):\n",
    "        self.sigma =sigma\n",
    "        \n",
    "    def W(self,  n_nodes1, n_nodes2):\n",
    "      \n",
    "        self.sigma = 1/np.sqrt(n_nodes1)\n",
    "        W = self.sigma *np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self,n_nodes2):\n",
    "        B = self.sigma *np.random.randn(n_nodes2)\n",
    "        return B\n",
    "\n",
    "class HeInitializer:\n",
    "    def __init__(self,sigma):\n",
    "        self.sigma =sigma\n",
    "    def W(self,  n_nodes1, n_nodes2):\n",
    "      \n",
    "        self.sigma = np.sqrt(2/n_nodes1)\n",
    "        W = self.sigma *np.random.randn( n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self,n_nodes2):\n",
    "        B = self.sigma *np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Stochastic gradient descent\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : Learning rate\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        Update weights and biases for a layer\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : Instance of the layer before update\n",
    "        \"\"\"\n",
    "        layer.W = layer.W - self.lr*layer.dW\n",
    "        layer.B = layer.B - self.lr*layer.dB\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, A): \n",
    "        Z = np.exp(A) / np.sum(np.exp(A), axis=1).reshape(-1, 1)\n",
    "        return Z\n",
    "        \n",
    "    def backward(self, Z, y):\n",
    "        dA = Z - y\n",
    "\n",
    "        return dA\n",
    "    def loss(self,Z,y):\n",
    "        L = - np.sum(y * np.log(Z)) / len(y)\n",
    "        return L\n",
    "    \n",
    "class ReLU:\n",
    "    def forward(self, A): \n",
    "        self.A = A\n",
    "        A[A <= 0] = 0\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dA = dZ*np.array(self.A > 0, np.int)\n",
    "        return dA\n",
    "    \n",
    "class Sigmoid:\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = 1 / (1 + np.exp(-self.A))\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * ((1 / (1 + np.exp(-self.A))) - (1 / (1 + np.exp(-self.A)))**2)\n",
    "        return dA\n",
    "    \n",
    "class Tanh:\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        Z = np.tanh(self.A)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * (1 - np.tanh(self.A)**2)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    Number of nodes Fully connected layer from n_nodes1 to n_nodes2\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      Number of nodes in the previous layer\n",
    "    n_nodes2 : int\n",
    "      Number of nodes in the later layer\n",
    "    initializer: instance of initialization method\n",
    "    optimizer: instance of optimization method\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # Initialize\n",
    "        # Initialize self.W and self.B using the initializer method\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.W = initializer.W(n_nodes1,n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.Hw = 0\n",
    "        self.Hb = 0\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        forward\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
    "            output\n",
    "        \"\"\"        \n",
    "#         print(X.shape)\n",
    "        A =  X @self.W + self.B\n",
    "        self.Z = X\n",
    "        return A\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Backward\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : The following forms of ndarray, shape (batch_size, n_nodes2)\n",
    "            Gradient flowing from behind\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : The following forms of ndarray, shape (batch_size, n_nodes1)\n",
    "            Gradient to flow forward\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA,axis = 0)\n",
    "        self.dW = self.Z.T@dA\n",
    "        self.dZ = dA @ self.W.T\n",
    "        \n",
    "        # update\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_size_calculation(n_in, P, F, S):\n",
    "    n_out = int((n_in + 2*P - F) / S + 1)\n",
    "    return n_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchConv1Classifier():\n",
    "    \"\"\"\n",
    "    Simple three-layer neural network classifier\n",
    "    Parameters\n",
    "    ----------\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self,n_nodes2, n_output ,batch_size ,epochs,sigma=0.02,optimizer=SGD,activation_function = \"sigmoid\",lr = 0.01,bias = False,verbose = True):\n",
    "        self.verbose = verbose\n",
    "#         self.n_nodes1= n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.n_output = n_output \n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "\n",
    "        self.check_bias = bias\n",
    "        self.activation_function = activation_function\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optimizer(self.lr)\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        self.initializer = self.get_initializer()\n",
    "#         print(X.shape[-1])\n",
    "        self.conv1d = Conv1d(kernel_size=7, initializer=SimpleInitializer(self.sigma), optimizer=self.optimizer, input_channels=1, output_channels=1, padding=3, stride=2)\n",
    "        nodes_1 = output_size_calculation(X.shape[-1], self.conv1d.padding, self.conv1d.kernel_size, self.conv1d.stride)\n",
    "        self.FC2 = FC(nodes_1,self.n_nodes2,self.initializer,self.optimizer)\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output,self.initializer,self.optimizer)\n",
    "        \n",
    "        self.activation1 = self.get_activation()\n",
    "        self.activation2 = self.get_activation()\n",
    "        self.activation3 = Softmax()\n",
    "            \n",
    "        get_mini_batch = GetMiniBatch(X,y,self.batch_size)\n",
    "        self.loss_train = []\n",
    "        self.loss_val = []\n",
    "        for epoch in range(self.epochs):  \n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "\n",
    "                self.forward(mini_X_train)\n",
    "#                \n",
    "                self.backward(mini_X_train,mini_y_train)\n",
    "#             break\n",
    "            self.forward(X)\n",
    "            self.loss_train.append(self.activation3.loss(self.result,y))\n",
    "            if X_val is not None:\n",
    "                self.forward(X_val)\n",
    "                self.loss_val.append(self.activation3.loss(self.result,y_val))\n",
    "            \n",
    "\n",
    "    \n",
    "        \n",
    "        \"\"\"\n",
    "        Learn a neural network classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of training data\n",
    "        y : The following form of ndarray, shape (n_samples,)\n",
    "            Correct answer value of training data\n",
    "        X_val : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            Features of verification data\n",
    "        y_val : The following form of ndarray, shape (n_samples,)\n",
    "            Correct value of verification data\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.verbose:\n",
    "             #verbose is set to True, the learning process etc. is output.\n",
    "            print(self.loss_train)\n",
    "    def get_initializer(self):\n",
    "        if self.activation_function == \"sigmoid\" or self.activation_function == \"tanh\":\n",
    "            return XavierInitializer(self.sigma)\n",
    "        elif self.activation_function == \"relu\":\n",
    "            return HeInitializer(self.sigma)\n",
    "    \n",
    "    def get_activation(self):\n",
    "        if self.activation_function == \"sigmoid\" :\n",
    "            return Sigmoid()\n",
    "        elif self.activation_function == \"tanh\":\n",
    "            return Tanh()\n",
    "        elif self.activation_function == \"relu\":\n",
    "            return ReLU()\n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "       \n",
    "    def forward(self,X):\n",
    "        A1 = self.conv1d.forward(X)\n",
    "        A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        self.result = Z3\n",
    "\n",
    "        \n",
    "\n",
    "    def backward(self,X,y):\n",
    "#         print(self.result.shape)\n",
    "#         print(y.shape)\n",
    "        dA3 = self.activation3.backward(self.result,y)\n",
    "        dZ2 = self.FC3.backward(dA3)\n",
    "        dA2 = self.activation2.backward(dZ2)\n",
    "        dZ1 = self.FC2.backward(dA2)\n",
    "        dA1 = self.activation1.backward(dZ1)\n",
    "        dA1 = dA1.reshape(dA1.shape[0],1, dA1.shape[-1])\n",
    "        dZ0 = self.conv1d.backward(dA1)\n",
    "       \n",
    "        \n",
    "\n",
    "    def derivative(self,A,d):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            dA = d*(1 / (1 + np.exp(-A)))*(1-(1 / (1 + np.exp(-A))))\n",
    "            return dA\n",
    "        elif self.activation_function == \" tanh\":\n",
    "            dA = d*(1- np.tanh(A)**2)\n",
    "            return dA\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Estimate using a neural network classifier.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "            sample\n",
    "        Returns\n",
    "        -------\n",
    "            The following form of ndarray, shape (n_samples, 1)\n",
    "            Estimated result\n",
    "        \"\"\"\n",
    "        self.forward(X)\n",
    "        return np.argmax(self.result,axis = 1)\n",
    "        \n",
    "      \n",
    "    \n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "Iterator to get a mini-batch\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
    "      Training data\n",
    "    y : The following form of ndarray, shape (n_samples, 1)\n",
    "      Correct answer value\n",
    "    batch_size : int\n",
    "      Batch size\n",
    "    seed : int\n",
    "      NumPy random number seed\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ScratchConv1Classifier(200, 10 ,20 ,10,activation_function = \"sigmoid\",lr = 0.01,bias = False,verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2897961954026031, 0.22288846466443443, 0.1837572432762507, 0.15673705187205925, 0.13620251092862787, 0.12035999688190413, 0.10811009247019333, 0.09845448952728304, 0.09057453369556255, 0.08390234733925006]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_new, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (result == y_test).sum()*100/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.86"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
